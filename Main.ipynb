{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":6120,"sourceType":"modelInstanceVersion","modelInstanceId":4603,"modelId":2797},{"sourceId":6125,"sourceType":"modelInstanceVersion","modelInstanceId":4596,"modelId":2797},{"sourceId":6127,"sourceType":"modelInstanceVersion","modelInstanceId":4598,"modelId":2797}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":602.064687,"end_time":"2024-08-24T00:33:30.797234","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-08-24T00:23:28.732547","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1487ca0f12e54416813264d376a5bca7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2947551bcf0a408186aae67f6f2747e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db3e0eb39e814af693359957fbab5580","IPY_MODEL_5321a688cc274e38af90fbf1a691d531","IPY_MODEL_756f9426fcc04add9c6914edf427c2cb"],"layout":"IPY_MODEL_31141aa3a0124c0ba42d1e48f2648497"}},"2cb8ef13decc4d95addfcf74ee23f5aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31141aa3a0124c0ba42d1e48f2648497":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36063ac8dada4464b70ffbfbfc58913b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cb8ef13decc4d95addfcf74ee23f5aa","max":1266,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d118563083b84225b3ed7a1ff792ba99","value":1266}},"4520fc00683b4a8f85b0f26c1aa28ddb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4584f9bddd5941119aef43c0eeda38e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dbfb59502ba47e4bc75379f767ea1f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9dee742323a94990b37a5f0d57d840ff","IPY_MODEL_790c478401f847b29685e10b9c046249","IPY_MODEL_cb1d363b494e48ba9e327c452d5ade48"],"layout":"IPY_MODEL_c1e2f87bdd474ae6ae78113cab3c83e2"}},"5321a688cc274e38af90fbf1a691d531":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c6128011d7f4cb18dfeffafaea29492","max":4706,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e3c60bbf72241fd87644c619893183d","value":4706}},"60f300155bd346ae81eb32e7087ede2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6908d58f86004f188b8bf5b6a285eb01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b1f321c26cf49dda200209b99336f90":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"756f9426fcc04add9c6914edf427c2cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4584f9bddd5941119aef43c0eeda38e2","placeholder":"â€‹","style":"IPY_MODEL_cdfc717020e341a0b8a70b9f5b608c3f","value":"â€‡4706/4706â€‡[00:19&lt;00:00,â€‡302.99it/s]"}},"790c478401f847b29685e10b9c046249":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6908d58f86004f188b8bf5b6a285eb01","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b1f321c26cf49dda200209b99336f90","value":3}},"8e3c60bbf72241fd87644c619893183d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8fda110e09a84dd4b88e6197eba1988c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"982090bddcb94763afb17af5da403ccf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c6128011d7f4cb18dfeffafaea29492":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dee742323a94990b37a5f0d57d840ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4520fc00683b4a8f85b0f26c1aa28ddb","placeholder":"â€‹","style":"IPY_MODEL_b7af3470a2ac47a0b51ac9670bee269e","value":"Loadingâ€‡Imagesâ€‡:â€‡100%"}},"9f262140e7d543ab90f61a0f239c288b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9cf9946204d426eaf1c11793d5f65cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9dec529fbe142e4bf4881e2e73a0fcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4cfcc31d589452d94662b08cb7e3dc3","IPY_MODEL_36063ac8dada4464b70ffbfbfc58913b","IPY_MODEL_b0dc0ae740374a31953b3dee0a3ef173"],"layout":"IPY_MODEL_60f300155bd346ae81eb32e7087ede2b"}},"b0dc0ae740374a31953b3dee0a3ef173":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1487ca0f12e54416813264d376a5bca7","placeholder":"â€‹","style":"IPY_MODEL_d2d9168680094a63969bc43a9fab5d81","value":"â€‡1266/1266â€‡[00:04&lt;00:00,â€‡319.02it/s]"}},"b7af3470a2ac47a0b51ac9670bee269e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1e2f87bdd474ae6ae78113cab3c83e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4cfcc31d589452d94662b08cb7e3dc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f262140e7d543ab90f61a0f239c288b","placeholder":"â€‹","style":"IPY_MODEL_8fda110e09a84dd4b88e6197eba1988c","value":"Loadingâ€‡Imagesâ€‡:â€‡100%"}},"cb1d363b494e48ba9e327c452d5ade48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e31a94f2f53e48889936e811bd87b4e5","placeholder":"â€‹","style":"IPY_MODEL_db221720acdd4225831461447475982d","value":"â€‡3/3â€‡[00:00&lt;00:00,â€‡138.00it/s]"}},"cdfc717020e341a0b8a70b9f5b608c3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d118563083b84225b3ed7a1ff792ba99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2d9168680094a63969bc43a9fab5d81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db221720acdd4225831461447475982d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db3e0eb39e814af693359957fbab5580":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_982090bddcb94763afb17af5da403ccf","placeholder":"â€‹","style":"IPY_MODEL_a9cf9946204d426eaf1c11793d5f65cb","value":"Loadingâ€‡Imagesâ€‡:â€‡100%"}},"e31a94f2f53e48889936e811bd87b4e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ISIC 2024 - Skin Lesion Detection with 3D-TB using KerasCV and Keras\n\n> The goal of this competition is to detect skin cancers in lesions cropped from 3D total body photographs.\n\n<div align=\"center\">\n  <image src=\"https://i.ibb.co/PYLN6QR/isic2024.jpg\">\n</div>\n\nThis notebook guides through training and deploying a Deep Learning model for skin cancer detection using skin lesion data from 3D total body photographs. Specifically, we'll employ the EfficientNetV2 backbone from KerasCV on the competition dataset. The notebook integrates both image data and tabular features (e.g., age, sex) to enhance skin cancer detection.\n\n**Fun fact:** This notebook is backend-agnostic, supporting TensorFlow, PyTorch, and JAX. Leveraging KerasCV and Keras allows flexibility in choosing the preferred backend. Explore more details on [Keras](https://keras.io/keras_core/announcement/).\n\nIn this notebook, following lessions will be covered:\n\n- Designing a data pipeline for a multi-input model.\n- Creating a random augmentation pipeline with KerasCV.\n- Efficiently loading data using [`tf.data`](https://www.tensorflow.org/guide/data).\n- Utilizing KerasCV presets to build the model.\n- Training the model.\n- Performing inference and generating submissions on testing data.\n\n**Note:** For a deeper understanding of KerasCV, refer to the [KerasCV guides](https://keras.io/guides/keras_cv/).","metadata":{"_uuid":"8744329c-418d-47df-be9c-0526e5e6f45c","_cell_guid":"bc2030f6-3d7f-4e49-bb0f-23d8d3c8336a","trusted":true,"collapsed":false,"papermill":{"duration":0.017796,"end_time":"2024-08-24T00:23:31.644148","exception":false,"start_time":"2024-08-24T00:23:31.626352","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# ðŸ“š | Import Libraries","metadata":{"_uuid":"d0e30946-b4ea-438d-9cd8-60f8cdaedba7","_cell_guid":"faf4cfb1-70cd-4c9d-8a55-08a15663112a","trusted":true,"collapsed":false,"papermill":{"duration":0.016228,"end_time":"2024-08-24T00:23:31.677001","exception":false,"start_time":"2024-08-24T00:23:31.660773","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\" # other options: tensorflow or torch\n\nimport keras_cv\nimport keras\nfrom keras import ops\nimport tensorflow as tf\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport joblib\n\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"a4d22339-36f8-41dc-85a6-4092ea235860","_cell_guid":"2f3457c7-ea4f-4d4e-8f32-0ec415ad9a01","trusted":true,"collapsed":false,"papermill":{"duration":18.334972,"end_time":"2024-08-24T00:23:50.028906","exception":false,"start_time":"2024-08-24T00:23:31.693934","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:29:00.446173Z","iopub.execute_input":"2025-04-26T16:29:00.446532Z","iopub.status.idle":"2025-04-26T16:29:17.301943Z","shell.execute_reply.started":"2025-04-26T16:29:00.446492Z","shell.execute_reply":"2025-04-26T16:29:17.301245Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Library Versions","metadata":{"_uuid":"675dd19a-93e5-4ca1-b27f-4b33b9422f41","_cell_guid":"1820d5c6-1451-4ac5-bb1a-2144bed5a719","trusted":true,"collapsed":false,"papermill":{"duration":0.017136,"end_time":"2024-08-24T00:23:50.063500","exception":false,"start_time":"2024-08-24T00:23:50.046364","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\"TensorFlow:\", tf.__version__)\nprint(\"Keras:\", keras.__version__)\nprint(\"KerasCV:\", keras_cv.__version__)","metadata":{"_uuid":"ae7f7306-59d1-4e5d-a0b7-b749c7c48fe0","_cell_guid":"a6797e98-e6db-46db-807f-bf2e0659179c","trusted":true,"collapsed":false,"papermill":{"duration":0.026379,"end_time":"2024-08-24T00:23:50.106978","exception":false,"start_time":"2024-08-24T00:23:50.080599","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:29:17.303523Z","iopub.execute_input":"2025-04-26T16:29:17.304019Z","iopub.status.idle":"2025-04-26T16:29:17.308998Z","shell.execute_reply.started":"2025-04-26T16:29:17.303990Z","shell.execute_reply":"2025-04-26T16:29:17.307981Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# âš™ï¸ | Configuration","metadata":{"_uuid":"14cafb18-b8dc-424d-84e3-823a99452e4f","_cell_guid":"fc849778-c7e3-44d6-93f4-30d7ad4ad069","trusted":true,"collapsed":false,"papermill":{"duration":0.01684,"end_time":"2024-08-24T00:23:50.141029","exception":false,"start_time":"2024-08-24T00:23:50.124189","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class CFG:\n    verbose = 1  # Verbosity\n    seed = 42  # Random seed\n    neg_sample = 0.01 # Downsample negative calss\n    pos_sample = 5.0  # Upsample positive class\n    preset = \"efficientnetv2_b2_imagenet\"  # Name of pretrained classifier\n    image_size = [128, 128]  # Input image size\n    epochs = 8 # Training epochs\n    batch_size = 8  # Batch size\n    lr_mode = \"cos\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n    class_names = ['target']\n    num_classes = 1","metadata":{"_uuid":"98b215bf-3c4a-492b-a5ba-e28d112a0f0c","_cell_guid":"b920d7a4-6ad1-4456-8a04-1cbf384f54c5","trusted":true,"collapsed":false,"papermill":{"duration":0.025831,"end_time":"2024-08-24T00:23:50.183771","exception":false,"start_time":"2024-08-24T00:23:50.157940","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:29:17.310248Z","iopub.execute_input":"2025-04-26T16:29:17.310480Z","iopub.status.idle":"2025-04-26T16:29:17.322387Z","shell.execute_reply.started":"2025-04-26T16:29:17.310457Z","shell.execute_reply":"2025-04-26T16:29:17.321490Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# â™»ï¸ | Reproducibility \nSets value for random seed to produce similar result in each run.","metadata":{"_uuid":"d5c75637-5bdd-4196-9abf-37722a653eda","_cell_guid":"0b23f173-73f1-493c-9878-87c8b18362d2","trusted":true,"collapsed":false,"papermill":{"duration":0.016545,"end_time":"2024-08-24T00:23:50.217205","exception":false,"start_time":"2024-08-24T00:23:50.200660","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"_uuid":"6163dc32-53b6-4f2c-a35b-63c3a0d8ee2e","_cell_guid":"14899877-71d3-4ec0-940d-e916ec79d399","trusted":true,"collapsed":false,"papermill":{"duration":0.025357,"end_time":"2024-08-24T00:23:50.259691","exception":false,"start_time":"2024-08-24T00:23:50.234334","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:29:17.323383Z","iopub.execute_input":"2025-04-26T16:29:17.323637Z","iopub.status.idle":"2025-04-26T16:29:17.335148Z","shell.execute_reply.started":"2025-04-26T16:29:17.323613Z","shell.execute_reply":"2025-04-26T16:29:17.334341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“ | Dataset Path","metadata":{"_uuid":"8bf45b1d-4196-42b1-80f2-4aa333f5ba1a","_cell_guid":"e50b2474-6b50-481d-bc4b-0d343c5519c4","trusted":true,"collapsed":false,"papermill":{"duration":0.016669,"end_time":"2024-08-24T00:23:50.293364","exception":false,"start_time":"2024-08-24T00:23:50.276695","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"BASE_PATH = \"/kaggle/input/isic-2024-challenge\"","metadata":{"_uuid":"85ff85dd-b4b1-44d6-8441-863c2d3d7c0e","_cell_guid":"69c32806-b5c8-417b-849a-de97886e3684","trusted":true,"collapsed":false,"papermill":{"duration":0.025171,"end_time":"2024-08-24T00:23:50.348662","exception":false,"start_time":"2024-08-24T00:23:50.323491","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:29:17.336924Z","iopub.execute_input":"2025-04-26T16:29:17.337194Z","iopub.status.idle":"2025-04-26T16:29:17.345616Z","shell.execute_reply.started":"2025-04-26T16:29:17.337151Z","shell.execute_reply":"2025-04-26T16:29:17.344925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“– | Meta Data\n\nIn this dataset, following information is available:\n\n- **train-image/**: Contains image files for the training set (provided for train only).\n- **train-image.hdf5**: Training image data stored in a single HDF5 file, where each image is indexed by its `isic_id`.\n- **train-metadata.csv**: Metadata corresponding to the training set, including:\n  - `isic_id`: Unique image ID used to query images in the HDF5 file.\n  - `patient_id`: Unique patient ID.\n  - `sex`: Gender of the patient.\n  - `age_approx`: Approximate age of the patient.\n  - `anatom_site_general`: Location of the lesion.\n  - Other relevant metadata fields.\n- **test-image.hdf5**: Testing image data stored in a single HDF5 file, initially containing 3 testing examples to validate the inference pipeline. Upon notebook submission, this file is replaced with the full hidden testing set, which contains approximately 500,000 images.\n- **test-metadata.csv**: Metadata corresponding to the testing subset.","metadata":{"_uuid":"3c31e94d-0397-495e-bad5-17d4c7c03756","_cell_guid":"376ed7b6-ce2a-4464-a9bb-d652c2341615","trusted":true,"collapsed":false,"papermill":{"duration":0.017251,"end_time":"2024-08-24T00:23:50.383241","exception":false,"start_time":"2024-08-24T00:23:50.365990","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Train + Valid\ndf = pd.read_csv(f'{BASE_PATH}/train-metadata.csv')\ndf = df.ffill()\ndisplay(df.head(2))\n\n# Testing\ntesting_df = pd.read_csv(f'{BASE_PATH}/test-metadata.csv')\ntesting_df = testing_df.ffill()\ndisplay(testing_df.head(2))","metadata":{"_uuid":"a64888de-c26c-47d1-8cea-c76d5833fe83","_cell_guid":"99f2504d-f3c7-425c-be34-e0b9b8d96f07","trusted":true,"collapsed":false,"papermill":{"duration":9.56639,"end_time":"2024-08-24T00:23:59.966363","exception":false,"start_time":"2024-08-24T00:23:50.399973","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:29:31.776656Z","iopub.execute_input":"2025-04-26T16:29:31.777595Z","iopub.status.idle":"2025-04-26T16:29:40.595873Z","shell.execute_reply.started":"2025-04-26T16:29:31.777557Z","shell.execute_reply":"2025-04-26T16:29:40.595042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"_uuid":"9189b522-f218-4cb7-9cd4-dd447a4db105","_cell_guid":"6ccaf85c-f61e-4488-95f0-4af1fbcebe01","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-26T16:31:27.081443Z","iopub.execute_input":"2025-04-26T16:31:27.081817Z","iopub.status.idle":"2025-04-26T16:31:27.087875Z","shell.execute_reply.started":"2025-04-26T16:31:27.081785Z","shell.execute_reply":"2025-04-26T16:31:27.086973Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# âš–ï¸ | Handle Class Imbalance","metadata":{"_uuid":"8c9049bf-ee60-450f-a4ca-f5aa40df607b","_cell_guid":"7a89cf30-3097-444f-abf3-03f8a0b3dc96","trusted":true,"collapsed":false,"papermill":{"duration":0.01743,"end_time":"2024-08-24T00:24:00.003338","exception":false,"start_time":"2024-08-24T00:23:59.985908","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Sample Data\n\nThere is a significant class imbalance in the dataset, with a large number of negative samples compared to positive samples. To address this issue, the negative class will be downsampled and upsample the positive class. To experiment with full dataset, simply adjust the `pos_sample` and `neg_sample` settings in `CFG`.","metadata":{"_uuid":"7b498752-baac-4769-af10-0c1e71252e2c","_cell_guid":"45e97ba3-3530-47e0-b1ba-41932d947da3","trusted":true,"collapsed":false,"papermill":{"duration":0.017177,"end_time":"2024-08-24T00:24:00.038195","exception":false,"start_time":"2024-08-24T00:24:00.021018","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\"Class Distribution Before Sampling (%):\")\ndisplay(df.target.value_counts(normalize=True)*100)\n\n# Sampling\npositive_df = df.query(\"target==0\").sample(frac=CFG.neg_sample, random_state=CFG.seed)\nnegative_df = df.query(\"target==1\").sample(frac=CFG.pos_sample, replace=True, random_state=CFG.seed)\ndf = pd.concat([positive_df, negative_df], axis=0).sample(frac=1.0)\n\nprint(\"\\nCalss Distribution After Sampling (%):\")\ndisplay(df.target.value_counts(normalize=True)*100)","metadata":{"_uuid":"a0490009-bfce-4bc0-a59e-581ee74a7412","_cell_guid":"e229d61a-a862-4890-a900-837fdbc30590","trusted":true,"collapsed":false,"_kg_hide-input":true,"papermill":{"duration":0.291517,"end_time":"2024-08-24T00:24:00.347304","exception":false,"start_time":"2024-08-24T00:24:00.055787","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:29:42.573024Z","iopub.execute_input":"2025-04-26T16:29:42.573861Z","iopub.status.idle":"2025-04-26T16:29:42.794490Z","shell.execute_reply.started":"2025-04-26T16:29:42.573824Z","shell.execute_reply":"2025-04-26T16:29:42.793604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Class Weight\n\nEven after downsampling the negative class and upsampling the positive class, there remains a significant class imbalance. To further address this imbalance during training, loss weighting will be used. This technique ensures that the model weights are updated more heavily for the positive samples, thereby reducing the bias towards the negative class. The following code computes the class weights for the loss:","metadata":{"_uuid":"1a6c8f35-28ea-466a-a49a-cce9451cffd1","_cell_guid":"dca49b8e-115f-4a3f-a466-bcb49bd3360a","trusted":true,"collapsed":false,"papermill":{"duration":0.017564,"end_time":"2024-08-24T00:24:00.384467","exception":false,"start_time":"2024-08-24T00:24:00.366903","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n# Assume df is your DataFrame and 'target' is the column with class labels\nclass_weights = compute_class_weight('balanced', classes=np.unique(df['target']), y=df['target'])\nclass_weights = dict(enumerate(class_weights))\nprint(\"Class Weights:\", class_weights)","metadata":{"_uuid":"a123457f-8493-4486-8fc3-359a06b43503","_cell_guid":"34c6a7c6-e4ce-4838-826d-d6b1e5c76366","trusted":true,"collapsed":false,"papermill":{"duration":1.022261,"end_time":"2024-08-24T00:24:01.424529","exception":false,"start_time":"2024-08-24T00:24:00.402268","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:29:44.633683Z","iopub.execute_input":"2025-04-26T16:29:44.634031Z","iopub.status.idle":"2025-04-26T16:29:44.928265Z","shell.execute_reply.started":"2025-04-26T16:29:44.634001Z","shell.execute_reply":"2025-04-26T16:29:44.927367Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ–¼ï¸ | Load Image Byte String\n\nIn this competition, images are provided as byte strings. The following code snippet demonstrates how to load these images into memory. One might wonder why the provided `jpeg` images aren't being used in the `/train-image` folder for training. This is because testing images are not provided as JPEG images; instead, they are provided as byte strings. Why use byte strings? They occupy significantly less memory compared to `np.array` representations.","metadata":{"_uuid":"7540fd77-e49e-43d9-a0e4-eeabce9dc46e","_cell_guid":"8a6da095-2a0a-439f-91fd-790f763ad0f6","trusted":true,"collapsed":false,"papermill":{"duration":0.017499,"end_time":"2024-08-24T00:24:01.460041","exception":false,"start_time":"2024-08-24T00:24:01.442542","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import h5py\n\ntraining_validation_hdf5 = h5py.File(f\"{BASE_PATH}/train-image.hdf5\", 'r')\ntesting_hdf5 = h5py.File(f\"{BASE_PATH}/test-image.hdf5\", 'r')","metadata":{"_uuid":"4f02b3fe-c77b-4a91-a7a3-1087d57cd8b0","_cell_guid":"02d69083-16b0-4739-8dde-e8972132454d","trusted":true,"collapsed":false,"papermill":{"duration":0.036408,"end_time":"2024-08-24T00:24:01.514455","exception":false,"start_time":"2024-08-24T00:24:01.478047","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:29:47.241360Z","iopub.execute_input":"2025-04-26T16:29:47.242019Z","iopub.status.idle":"2025-04-26T16:29:47.275456Z","shell.execute_reply.started":"2025-04-26T16:29:47.241981Z","shell.execute_reply":"2025-04-26T16:29:47.274204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check Image\n\nExamining a sample image from the provided data is important. This step allows for a closer inspection of the image quality and content, ensuring it meets the requirements for further processing.","metadata":{"_uuid":"98782d2e-9595-472f-9d29-84e529d9c064","_cell_guid":"f9a1ede9-f3b6-484f-a11f-c6164064b62c","trusted":true,"collapsed":false,"papermill":{"duration":0.01778,"end_time":"2024-08-24T00:24:01.550380","exception":false,"start_time":"2024-08-24T00:24:01.532600","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"isic_id = df.isic_id.iloc[0]\n\n# Image as Byte String\nbyte_string = training_validation_hdf5[isic_id][()]\nprint(f\"Byte String: {byte_string[:20]}....\")\n\n# Convert byte string to numpy array\nnparr = np.frombuffer(byte_string, np.uint8)\n\nprint(\"Image:\")\nimage = cv2.imdecode(nparr, cv2.IMREAD_COLOR)[...,::-1] # reverse last axis for bgr -> rgb\nplt.imshow(image);","metadata":{"_uuid":"e26b2147-aa78-491d-bb19-5bd23e5bfff3","_cell_guid":"78c2bfa8-454f-44aa-ad53-54a2996a6782","trusted":true,"collapsed":false,"papermill":{"duration":0.507857,"end_time":"2024-08-24T00:24:02.076049","exception":false,"start_time":"2024-08-24T00:24:01.568192","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:29:48.370658Z","iopub.execute_input":"2025-04-26T16:29:48.371348Z","iopub.status.idle":"2025-04-26T16:29:48.813789Z","shell.execute_reply.started":"2025-04-26T16:29:48.371314Z","shell.execute_reply":"2025-04-26T16:29:48.812927Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ”ª | Data Split\n\nIn the following code,the data will be splitted into `5` stratified folds and use the first fold for training and validation. It's important to note that `StratifiedGroupKFold` is being used to ensure that `patient_id`s do not overlap between the training and validation datasets. This prevents data leakage, where the model could potentially peak at data it should not have access to.\n\n> **Note**: Data leakage can lead to artificially high validation scores that do not reflect real-world performance.","metadata":{"_uuid":"fdb3b172-de3f-4cb0-8058-d2e08593a95e","_cell_guid":"7ce22da5-8c2e-40f6-8ef8-3bb4837662e8","trusted":true,"collapsed":false,"execution":{"iopub.execute_input":"2024-06-30T05:46:56.020823Z","iopub.status.busy":"2024-06-30T05:46:56.019942Z","iopub.status.idle":"2024-06-30T05:46:56.844037Z","shell.execute_reply":"2024-06-30T05:46:56.843042Z","shell.execute_reply.started":"2024-06-30T05:46:56.020788Z"},"papermill":{"duration":0.019778,"end_time":"2024-08-24T00:24:02.116523","exception":false,"start_time":"2024-08-24T00:24:02.096745","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedGroupKFold\n\ndf = df.reset_index(drop=True) # ensure continuous index\ndf[\"fold\"] = -1\nsgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=CFG.seed)\nfor i, (training_idx, validation_idx) in enumerate(sgkf.split(df, y=df.target, groups=df.patient_id)):\n    df.loc[validation_idx, \"fold\"] = int(i)\n\n# Use first fold for training and validation\ntraining_df = df.query(\"fold!=0\")\nvalidation_df = df.query(\"fold==0\")\nprint(f\"# Num Train: {len(training_df)} | Num Valid: {len(validation_df)}\")","metadata":{"_uuid":"27006d66-0c47-45d8-ab2f-25c41d8affef","_cell_guid":"8603a4fe-1e14-4782-b657-d592c9daadc3","trusted":true,"collapsed":false,"papermill":{"duration":0.533119,"end_time":"2024-08-24T00:24:02.669572","exception":false,"start_time":"2024-08-24T00:24:02.136453","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:33:14.278389Z","iopub.execute_input":"2025-04-26T16:33:14.278734Z","iopub.status.idle":"2025-04-26T16:33:14.609049Z","shell.execute_reply.started":"2025-04-26T16:33:14.278701Z","shell.execute_reply":"2025-04-26T16:33:14.608192Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Class Distribution in Training","metadata":{"_uuid":"3b76e2c8-5c22-4c50-ad11-1489e87977f9","_cell_guid":"c084e0e7-945e-4f6c-bdce-b6995e3f6f62","trusted":true,"collapsed":false,"papermill":{"duration":0.019792,"end_time":"2024-08-24T00:24:02.710591","exception":false,"start_time":"2024-08-24T00:24:02.690799","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"training_df.target.value_counts()","metadata":{"_uuid":"df406b5c-4dfb-48cd-ad74-d21492470ff0","_cell_guid":"09d44096-e3f4-4fec-a3fa-b5e0cf60295f","trusted":true,"collapsed":false,"papermill":{"duration":0.032588,"end_time":"2024-08-24T00:24:02.763181","exception":false,"start_time":"2024-08-24T00:24:02.730593","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:33:16.297487Z","iopub.execute_input":"2025-04-26T16:33:16.297845Z","iopub.status.idle":"2025-04-26T16:33:16.305443Z","shell.execute_reply.started":"2025-04-26T16:33:16.297812Z","shell.execute_reply":"2025-04-26T16:33:16.304314Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Class Distribution in Validation","metadata":{"_uuid":"20baba4a-98cc-4466-921b-6e1a8216d3ba","_cell_guid":"50b0acd9-7e27-482f-955e-6d5ae9fa21f9","trusted":true,"collapsed":false,"papermill":{"duration":0.020047,"end_time":"2024-08-24T00:24:02.803749","exception":false,"start_time":"2024-08-24T00:24:02.783702","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"validation_df.target.value_counts()","metadata":{"_uuid":"ae712447-73a0-4e3c-a497-238de81445c6","_cell_guid":"09eeccc8-5bcb-4db5-b5da-84c342d10a3f","trusted":true,"collapsed":false,"papermill":{"duration":0.031501,"end_time":"2024-08-24T00:24:02.855751","exception":false,"start_time":"2024-08-24T00:24:02.824250","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:33:17.846451Z","iopub.execute_input":"2025-04-26T16:33:17.847304Z","iopub.status.idle":"2025-04-26T16:33:17.855230Z","shell.execute_reply.started":"2025-04-26T16:33:17.847253Z","shell.execute_reply":"2025-04-26T16:33:17.854157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“Š | Tabular Features\n\nIn this competition, alongside image data, tabular features such as age, sex, and the location of the lesion are available. Previous competitions, like [ISIC 2020](https://www.kaggle.com/c/siim-isic-melanoma-classification/overview), have demonstrated that incorporating these tabular features can significantly enhance model performance. A similar improvement is anticipated here.\n\nThe following code snippet provides a method for selecting which tabular features to include. It is encouraged to experiment with various combinations to determine the most effective set.","metadata":{"_uuid":"303c6a43-a690-46f4-a7d2-a5c16f04c40c","_cell_guid":"5dc18f6f-d856-424d-a062-9e9c79f4beac","trusted":true,"collapsed":false,"papermill":{"duration":0.020298,"end_time":"2024-08-24T00:24:02.897461","exception":false,"start_time":"2024-08-24T00:24:02.877163","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Categorical features which will be one hot encoded\nCATEGORICAL_COLUMNS = [\"sex\", \"anatom_site_general\",\n            \"tbp_tile_type\",\"tbp_lv_location\", ]\n\n# Numeraical features which will be normalized\nNUMERIC_COLUMNS = [\"age_approx\", \"tbp_lv_nevi_confidence\", \"clin_size_long_diam_mm\",\n           \"tbp_lv_areaMM2\", \"tbp_lv_area_perim_ratio\", \"tbp_lv_color_std_mean\",\n           \"tbp_lv_deltaLBnorm\", \"tbp_lv_minorAxisMM\", ]\n\n# Tabular feature columns\nFEAT_COLS = CATEGORICAL_COLUMNS + NUMERIC_COLUMNS","metadata":{"_uuid":"f1093583-7770-4817-8c28-1665462e4554","_cell_guid":"595e8ee8-c4ff-46d7-9a86-37158f3b8e38","trusted":true,"collapsed":false,"papermill":{"duration":0.028923,"end_time":"2024-08-24T00:24:02.946707","exception":false,"start_time":"2024-08-24T00:24:02.917784","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T16:33:22.400156Z","iopub.execute_input":"2025-04-26T16:33:22.400544Z","iopub.status.idle":"2025-04-26T16:33:22.405396Z","shell.execute_reply.started":"2025-04-26T16:33:22.400514Z","shell.execute_reply":"2025-04-26T16:33:22.404360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df[FEAT_COLS])","metadata":{"_uuid":"0bbd63f1-29d1-4091-8cee-113b1e2d27d1","_cell_guid":"f9a24641-605b-4c32-a7f1-5fe02d657097","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-26T16:33:24.431482Z","iopub.execute_input":"2025-04-26T16:33:24.432245Z","iopub.status.idle":"2025-04-26T16:33:24.443271Z","shell.execute_reply.started":"2025-04-26T16:33:24.432210Z","shell.execute_reply":"2025-04-26T16:33:24.442412Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Generation","metadata":{"_uuid":"323d9ae6-73d7-4042-ad93-85db5957d6bc","_cell_guid":"942efe68-23c6-4943-8450-eedfa10d7f28","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Conditional VAE","metadata":{"_uuid":"20f45cfd-8050-4d3e-8786-84a258a21cd6","_cell_guid":"17fdd2a2-6bff-47b2-a168-5ca2f9bf5c96","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ===========================\n# 1. IMPORTS\n# ===========================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Lambda, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\n\n# ===========================\n# 2. DATA PREPROCESSING\n# ===========================\n\n# Assume your original df is loaded\nCATEGORICAL_COLUMNS = [\"sex\", \"anatom_site_general\", \"tbp_tile_type\", \"tbp_lv_location\"]\nNUMERIC_COLUMNS = [\"age_approx\", \"tbp_lv_nevi_confidence\", \"clin_size_long_diam_mm\",\n                   \"tbp_lv_areaMM2\", \"tbp_lv_area_perim_ratio\", \"tbp_lv_color_std_mean\",\n                   \"tbp_lv_deltaLBnorm\", \"tbp_lv_minorAxisMM\"]\n\ndf_original = df[CATEGORICAL_COLUMNS + NUMERIC_COLUMNS + ['target']]\n\n# One-hot encode categorical data\nohe = OneHotEncoder(sparse=False)\ncat_data = ohe.fit_transform(df_original[CATEGORICAL_COLUMNS])\n\n# MinMax scale numerical data\nscaler = MinMaxScaler()\nnum_data = scaler.fit_transform(df_original[NUMERIC_COLUMNS])\n\n# Get target\ntarget_data = df_original['target'].values.reshape(-1, 1)\n\n# Combine preprocessed features\nX = np.concatenate([cat_data, num_data], axis=1)\n\ninput_dim = X.shape[1]\nlatent_dim = 10  # Size of latent space\n\n# ===========================\n# 3. CVAE MODEL BUILDING\n# ===========================\n\n# ----- Encoder -----\nfeatures_input = Input(shape=(input_dim,))\ntarget_input = Input(shape=(1,))  # 0 or 1\n\n# Concatenate features + target\nx = Concatenate()([features_input, target_input])\nh = Dense(64, activation=\"relu\")(x)\nz_mean = Dense(latent_dim)(h)\nz_log_var = Dense(latent_dim)(h)\n\n# Sampling\ndef sampling(args):\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\nz = Lambda(sampling)([z_mean, z_log_var])\n\n# ----- Decoder -----\nz_cond = Concatenate()([z, target_input])\ndecoder_h = Dense(64, activation=\"relu\")\ndecoder_cat_out = Dense(cat_data.shape[1], activation=\"softmax\")  # Categorical\ndecoder_num_out = Dense(num_data.shape[1], activation=\"sigmoid\")  # Numerical\n\nh_decoded = decoder_h(z_cond)\nout_cat = decoder_cat_out(h_decoded)\nout_num = decoder_num_out(h_decoded)\noutputs = Concatenate()([out_cat, out_num])\n\n# Full CVAE model\ncvae = Model([features_input, target_input], outputs)\n\n# Loss\ncvae.compile(optimizer=\"adam\", loss=\"mse\")\n\n# Summary\ncvae.summary()\n\n# ===========================\n# 4. TRAIN THE CVAE\n# ===========================\n\ncvae.fit([X, target_data], X,\n         epochs=50,\n         batch_size=32,\n         validation_split=0.2,\n         verbose=1)\n\n# ===========================\n# 5. BUILD DECODER SEPARATELY\n# ===========================\n\nlatent_input = Input(shape=(latent_dim,))\ntarget_input_decoder = Input(shape=(1,))\n\nz_cond_dec = Concatenate()([latent_input, target_input_decoder])\nh_dec = decoder_h(z_cond_dec)\nout_cat_dec = decoder_cat_out(h_dec)\nout_num_dec = decoder_num_out(h_dec)\ndecoded_output = Concatenate()([out_cat_dec, out_num_dec])\n\ndecoder = Model([latent_input, target_input_decoder], decoded_output)\n\n# ===========================\n# 6. GENERATE SYNTHETIC DATA\n# ===========================\n\n# Generate for benign and malignant\nnum_samples = 1000  # Total synthetic samples\n\n# Half benign, half malignant\nz_sample_benign = np.random.normal(size=(num_samples // 2, latent_dim))\nz_sample_malignant = np.random.normal(size=(num_samples // 2, latent_dim))\n\ntarget_benign = np.zeros((num_samples // 2, 1))\ntarget_malignant = np.ones((num_samples // 2, 1))\n\n# Decode\ngenerated_benign = decoder.predict([z_sample_benign, target_benign])\ngenerated_malignant = decoder.predict([z_sample_malignant, target_malignant])\n\n# Combine\ngenerated = np.vstack([generated_benign, generated_malignant])\ntargets_generated = np.vstack([target_benign, target_malignant]).flatten()\n\nprint(\"\\nGenerated synthetic feature shape:\", generated.shape)\n\n# ===========================\n# 7. POSTPROCESS GENERATED DATA\n# ===========================\n\n# Separate categorical and numerical parts\ngenerated_cat = generated[:, :cat_data.shape[1]]\ngenerated_num = generated[:, cat_data.shape[1]:]\n\n# Harden categorical outputs correctly (group-wise)\ngenerated_cat_hardened = np.zeros_like(generated_cat)\ncategory_sizes = [len(c) for c in ohe.categories_]\n\nstart_idx = 0\nfor size in category_sizes:\n    end_idx = start_idx + size\n    \n    group = generated_cat[:, start_idx:end_idx]\n    group_hardened = np.zeros_like(group)\n    max_indices = np.argmax(group, axis=1)\n    group_hardened[np.arange(len(group)), max_indices] = 1\n    \n    generated_cat_hardened[:, start_idx:end_idx] = group_hardened\n    \n    start_idx = end_idx\n\n# Inverse transforms\noriginal_cat_labels = ohe.inverse_transform(generated_cat_hardened)\noriginal_num_data = scaler.inverse_transform(generated_num)\n\n# ===========================\n# 8. COMBINE INTO FINAL SYNTHETIC DATAFRAME\n# ===========================\n\ndf_cat = pd.DataFrame(original_cat_labels, columns=CATEGORICAL_COLUMNS)\ndf_num = pd.DataFrame(original_num_data, columns=NUMERIC_COLUMNS)\ndf_target = pd.DataFrame(targets_generated, columns=[\"target\"])\n\nsynthetic_df = pd.concat([df_cat, df_num, df_target], axis=1)\n\n# Save final synthetic dataset\nsynthetic_df.to_csv(\"synthetic_data_cvae_final.csv\", index=False)\n\nprint(\"\\nâœ… Final synthetic dataset created successfully: 'synthetic_data_cvae_final.csv'\")\nprint(synthetic_df.head())","metadata":{"_uuid":"d129232d-6803-4c77-9f73-d59fc5627151","_cell_guid":"44200a90-63fb-4afa-b906-a6fcc5ca4634","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-26T16:42:26.031660Z","iopub.execute_input":"2025-04-26T16:42:26.032311Z","iopub.status.idle":"2025-04-26T16:42:46.806481Z","shell.execute_reply.started":"2025-04-26T16:42:26.032277Z","shell.execute_reply":"2025-04-26T16:42:46.805571Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸš | DataLoader\n\nThis DataLoader is designed to process both `images` and tabular `features` simultaneously as inputs. It applies augmentations like `flip` and `cutout`, with additional options available such as random brightness, contrast, zoom, and rotation. Experimentation with different augmentations is encouraged. More details on the available augmentations in KerasCV can be found [here](https://keras.io/api/keras_cv/layers/preprocessing/).\n\n> Note: Unlike standard augmentations, these augmentations are applied to a batch, enhancing training speed and reducing CPU bottlenecks.","metadata":{"_uuid":"80e172ce-3cb4-4412-b95e-4e7008cca739","_cell_guid":"1d942bf3-bf71-4e4b-8d6e-6bebfc0dcfab","trusted":true,"collapsed":false,"papermill":{"duration":0.02017,"end_time":"2024-08-24T00:24:02.987306","exception":false,"start_time":"2024-08-24T00:24:02.967136","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def build_augmenter():\n    # Define augmentations\n    aug_layers = [\n        keras_cv.layers.RandomCutout(height_factor=(0.02, 0.06), width_factor=(0.02, 0.06)),\n        keras_cv.layers.RandomFlip(mode=\"horizontal\"),\n    ]\n    \n    # Apply augmentations to random samples\n    aug_layers = [keras_cv.layers.RandomApply(x, rate=0.5) for x in aug_layers]\n    \n    # Build augmentation layer\n    augmenter = keras_cv.layers.Augmenter(aug_layers)\n\n    # Apply augmentations\n    def augment(inp, label):\n        images = inp[\"images\"]\n        aug_data = {\"images\": images}\n        aug_data = augmenter(aug_data)\n        inp[\"images\"] = aug_data[\"images\"]\n        return inp, label\n    return augment\n\n\ndef build_decoder(with_labels=True, target_size=CFG.image_size):\n    def decode_image(inp):\n        # Read jpeg image\n        file_bytes = inp[\"images\"]\n        image = tf.io.decode_jpeg(file_bytes)\n        \n        # Resize\n        image = tf.image.resize(image, size=target_size, method=\"area\")\n        \n        # Rescale image\n        image = tf.cast(image, tf.float32)\n        image /= 255.0\n        \n        # Reshape\n        image = tf.reshape(image, [*target_size, 3])\n        \n        inp[\"images\"] = image\n        return inp\n\n    def decode_label(label, num_classes):\n        label = tf.cast(label, tf.float32)\n        label = tf.reshape(label, [num_classes])\n        return label\n\n    def decode_with_labels(inp, label=None):\n        inp = decode_image(inp)\n        label = decode_label(label, CFG.num_classes)\n        return (inp, label)\n\n    return decode_with_labels if with_labels else decode_image\n\n\ndef build_dataset(\n    isic_ids,\n    hdf5,\n    features,\n    labels=None,\n    batch_size=32,\n    decode_fn=None,\n    augment_fn=None,\n    augment=False,\n    shuffle=1024,\n    cache=True,\n    drop_remainder=False,\n):\n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n\n    if augment_fn is None:\n        augment_fn = build_augmenter()\n\n    AUTO = tf.data.experimental.AUTOTUNE\n\n    images = [None]*len(isic_ids)\n    for i, isic_id in enumerate(tqdm(isic_ids, desc=\"Loading Images \")):\n        images[i] = hdf5[isic_id][()]\n        \n    inp = {\"images\": images, \"features\": features}\n    slices = (inp, labels) if labels is not None else inp\n\n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.cache() if cache else ds\n    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n    if shuffle:\n        ds = ds.shuffle(shuffle, seed=CFG.seed)\n        opt = tf.data.Options()\n        opt.deterministic = False\n        ds = ds.with_options(opt)\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n    ds = ds.prefetch(AUTO)\n    return ds","metadata":{"_uuid":"9379bf95-1ef3-4a4d-a7ba-fc2daf056285","_cell_guid":"332327d0-8908-49d0-bcc7-ddb717cc6755","trusted":true,"collapsed":false,"papermill":{"duration":0.042596,"end_time":"2024-08-24T00:24:03.050235","exception":false,"start_time":"2024-08-24T00:24:03.007639","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T06:58:54.456573Z","iopub.execute_input":"2025-04-25T06:58:54.457155Z","iopub.status.idle":"2025-04-25T06:58:54.469018Z","shell.execute_reply.started":"2025-04-25T06:58:54.457122Z","shell.execute_reply":"2025-04-25T06:58:54.468165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build Training & Validation Dataset\n\nIn the following code, **training** and **validation** data loaders will be created.","metadata":{"_uuid":"af38e848-134a-474f-8496-57d0fbef6988","_cell_guid":"2381f5a1-5ac0-42d7-92dd-14e7b84563f9","trusted":true,"collapsed":false,"papermill":{"duration":0.020252,"end_time":"2024-08-24T00:24:03.091138","exception":false,"start_time":"2024-08-24T00:24:03.070886","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"## Train\nprint(\"# Training:\")\ntraining_features = dict(training_df[FEAT_COLS])\ntraining_ids = training_df.isic_id.values\ntraining_labels = training_df.target.values\ntraining_ds = build_dataset(training_ids, training_validation_hdf5, training_features, \n                         training_labels, batch_size=CFG.batch_size,\n                         shuffle=True, augment=True)\n\n# Valid\nprint(\"# Validation:\")\nvalidation_features = dict(validation_df[FEAT_COLS])\nvalidation_ids = validation_df.isic_id.values\nvalidation_labels = validation_df.target.values\nvalidation_ds = build_dataset(validation_ids, training_validation_hdf5, validation_features,\n                         validation_labels, batch_size=CFG.batch_size,\n                         shuffle=False, augment=False)","metadata":{"_uuid":"6c22b9b2-d815-47d8-a604-4b5651fbcce6","_cell_guid":"bab0ac3c-1954-49c8-b223-992bacdf9391","trusted":true,"collapsed":false,"papermill":{"duration":27.294922,"end_time":"2024-08-24T00:24:30.406449","exception":false,"start_time":"2024-08-24T00:24:03.111527","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T06:59:11.414002Z","iopub.execute_input":"2025-04-25T06:59:11.414941Z","iopub.status.idle":"2025-04-25T06:59:44.018256Z","shell.execute_reply.started":"2025-04-25T06:59:11.414892Z","shell.execute_reply":"2025-04-25T06:59:44.017568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(training_ds)","metadata":{"_uuid":"8975b343-0a87-42e5-b17a-3eea070ce9d3","_cell_guid":"9d23653a-1053-46f0-88f2-e8373bd5900c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-25T06:59:44.019825Z","iopub.execute_input":"2025-04-25T06:59:44.020111Z","iopub.status.idle":"2025-04-25T06:59:44.024707Z","shell.execute_reply.started":"2025-04-25T06:59:44.020084Z","shell.execute_reply":"2025-04-25T06:59:44.023940Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_space = keras.utils.FeatureSpace(\n    features={\n        # Categorical features encoded as integers\n        \"sex\": \"string_categorical\",\n        \"anatom_site_general\": \"string_categorical\",\n        \"tbp_tile_type\": \"string_categorical\",\n        \"tbp_lv_location\": \"string_categorical\",\n        # Numerical features to discretize\n        \"age_approx\": \"float_discretized\",\n        # Numerical features to normalize\n        \"tbp_lv_nevi_confidence\": \"float_normalized\",\n        \"clin_size_long_diam_mm\": \"float_normalized\",\n        \"tbp_lv_areaMM2\": \"float_normalized\",\n        \"tbp_lv_area_perim_ratio\": \"float_normalized\",\n        \"tbp_lv_color_std_mean\": \"float_normalized\",\n        \"tbp_lv_deltaLBnorm\": \"float_normalized\",\n        \"tbp_lv_minorAxisMM\": \"float_normalized\",\n    },\n    output_mode=\"concat\",\n)","metadata":{"_uuid":"e32c2b20-3515-4f1d-a15f-b976074aeaff","_cell_guid":"57103ab7-38cf-4dad-a959-a800097a4b59","trusted":true,"collapsed":false,"papermill":{"duration":0.082237,"end_time":"2024-08-24T00:24:30.557510","exception":false,"start_time":"2024-08-24T00:24:30.475273","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T06:59:48.200286Z","iopub.execute_input":"2025-04-25T06:59:48.200607Z","iopub.status.idle":"2025-04-25T06:59:48.231043Z","shell.execute_reply.started":"2025-04-25T06:59:48.200581Z","shell.execute_reply":"2025-04-25T06:59:48.230446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configuring a `FeatureSpace`\n\nTo set up how each tabular feature should be preprocessed, `keras.utils.FeatureSpace` is used. A dictionary is passsed to it that maps each feature name to a string describing its type.\n\n- **String Categorical Features**: \n  - Examples: `sex`, `anotm_site_general`\n- **Numerical Features**:\n  - Examples: `tbp_lv_nevi_confidence`, `clin_size_long_diam_mm`\n  - Note: These features will be normalized.\n- **Numerical Discrete Features**:\n  - `age_approx`: Need to discretize this feature into a number of bins.","metadata":{"_uuid":"609dc320-ee73-490b-b6b1-26f9016fbd6d","_cell_guid":"28dfefd6-5c62-4488-b744-6e6fcd1d2754","trusted":true,"collapsed":false,"papermill":{"duration":0.028887,"end_time":"2024-08-24T00:24:30.612429","exception":false,"start_time":"2024-08-24T00:24:30.583542","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Adapt Tabular Features\n\nBefore the `FeatureSpace` is used to build a model, it needs to be adapted to the training data. During `adapt()`, the `FeatureSpace` will:\n\n- Index the set of possible values for **categorical features**.\n- Compute the mean and variance for **numerical features** to normalize.\n- Compute the value boundaries for the different bins for **numerical features** to discretize.\n\nNote: `adapt()` should be called on a `tf.data.Dataset` that yields dictionaries of feature values â€“ no labels.","metadata":{"_uuid":"1bbf6ca7-f9ea-4617-9608-e35f26422c74","_cell_guid":"4518d45f-cb82-40e5-942e-88b0d0899a31","trusted":true,"collapsed":false,"papermill":{"duration":0.020592,"end_time":"2024-08-24T00:24:30.655103","exception":false,"start_time":"2024-08-24T00:24:30.634511","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"training_ds_with_no_labels = training_ds.map(lambda x, _: x[\"features\"])\nfeature_space.adapt(training_ds_with_no_labels)","metadata":{"_uuid":"4932ca46-3784-430f-b283-93a1d4d35eeb","_cell_guid":"853d5578-5da7-41e7-8949-0d93245d36b9","trusted":true,"collapsed":false,"papermill":{"duration":110.613656,"end_time":"2024-08-24T00:26:21.289648","exception":false,"start_time":"2024-08-24T00:24:30.675992","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T06:59:53.396324Z","iopub.execute_input":"2025-04-25T06:59:53.396652Z","iopub.status.idle":"2025-04-25T07:01:01.889060Z","shell.execute_reply.started":"2025-04-25T06:59:53.396622Z","shell.execute_reply":"2025-04-25T07:01:01.888361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"At this point, the `FeatureSpace` can be called on a dictionary of raw feature values. It will return a single concatenated vector for each sample, combining encoded features.\n\nIn the code below, it can be noticed that even though $12$ raw (tabular) features are used, after processing with `FeatureSpace`, a vector of size $71$ is created. This is because operations like one-hot encoding are being applied here.","metadata":{"_uuid":"b8a23803-ef45-45a8-b7b9-59d394077e5f","_cell_guid":"92eee6e3-f6f6-4371-b978-761abcc244a3","trusted":true,"collapsed":false,"papermill":{"duration":0.020907,"end_time":"2024-08-24T00:26:21.334914","exception":false,"start_time":"2024-08-24T00:26:21.314007","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"for x, _ in training_ds.take(1):\n    preprocessed_x = feature_space(x[\"features\"])\n    print(\"preprocessed_x.shape:\", preprocessed_x.shape)\n    print(\"preprocessed_x.dtype:\", preprocessed_x.dtype)","metadata":{"_uuid":"9cb7893b-238e-4fd9-9afe-4adfc42af59b","_cell_guid":"e0d1a179-bd8e-4c04-bb9f-ea59562b7ef1","trusted":true,"collapsed":false,"papermill":{"duration":2.193971,"end_time":"2024-08-24T00:26:23.550022","exception":false,"start_time":"2024-08-24T00:26:21.356051","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:01:01.890734Z","iopub.execute_input":"2025-04-25T07:01:01.891003Z","iopub.status.idle":"2025-04-25T07:01:02.639255Z","shell.execute_reply.started":"2025-04-25T07:01:01.890977Z","shell.execute_reply":"2025-04-25T07:01:02.638397Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Apply Feature Processing\n\nIntegrating feature space processing into the data pipeline before the model is crucial. This approach enables asynchronous, parallel preprocessing of data on the CPU, ensuring it is optimized before being fed into the model.","metadata":{"_uuid":"a0e08182-26f7-4899-a81a-afa1165e93a9","_cell_guid":"def8aa7a-70dc-47e9-a095-83138bb6e252","trusted":true,"collapsed":false,"papermill":{"duration":0.021384,"end_time":"2024-08-24T00:26:23.592643","exception":false,"start_time":"2024-08-24T00:26:23.571259","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"training_ds = training_ds.map(\n    lambda x, y: ({\"images\": x[\"images\"],\n                   \"features\": feature_space(x[\"features\"])}, y), num_parallel_calls=tf.data.AUTOTUNE)\n\nvalidation_ds = validation_ds.map(\n    lambda x, y: ({\"images\": x[\"images\"],\n                   \"features\": feature_space(x[\"features\"])}, y), num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"_uuid":"02c20f5c-e7f0-4cca-8804-d9d5865ba3d7","_cell_guid":"784e5968-b143-4ec8-a35e-5a8757f39800","trusted":true,"collapsed":false,"papermill":{"duration":0.303585,"end_time":"2024-08-24T00:26:23.917493","exception":false,"start_time":"2024-08-24T00:26:23.613908","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:01:02.640555Z","iopub.execute_input":"2025-04-25T07:01:02.640826Z","iopub.status.idle":"2025-04-25T07:01:02.866446Z","shell.execute_reply.started":"2025-04-25T07:01:02.640801Z","shell.execute_reply":"2025-04-25T07:01:02.865538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"b018296f-fcb1-41f7-83c6-7bf87f29ab7f","_cell_guid":"555618ac-471a-4f3a-a924-8a8b006b7e00","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Output Shape of a Batch\n\nVerifying the shape of a batch sample is essential. This step ensures that the dataloader is generating inputs with the correct dimensions, which is critical for the model's performance.","metadata":{"_uuid":"bc4d2848-4ff9-4697-abf0-424c1be2cb4f","_cell_guid":"265c97c9-269f-4889-80ab-fcfde6d4e954","trusted":true,"collapsed":false,"papermill":{"duration":0.020817,"end_time":"2024-08-24T00:26:23.959558","exception":false,"start_time":"2024-08-24T00:26:23.938741","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"batch = next(iter(validation_ds))\n\nprint(\"Images:\",batch[0][\"images\"].shape)\nprint(\"Features:\", batch[0][\"features\"].shape)\nprint(\"Targets:\", batch[1].shape)","metadata":{"_uuid":"07bb5d77-6832-4c66-afea-53cbf60952ca","_cell_guid":"d04f9852-f1b8-4329-90e1-388cbf8a5ee3","trusted":true,"collapsed":false,"papermill":{"duration":0.224381,"end_time":"2024-08-24T00:26:24.205078","exception":false,"start_time":"2024-08-24T00:26:23.980697","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:01:02.868040Z","iopub.execute_input":"2025-04-25T07:01:02.868340Z","iopub.status.idle":"2025-04-25T07:01:02.962609Z","shell.execute_reply.started":"2025-04-25T07:01:02.868315Z","shell.execute_reply":"2025-04-25T07:01:02.961769Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Check\n\nVisualizing samples along with their associated labels from the dataset is a vital step. This process helps in understanding the data distribution and ensures that the dataset is correctly labeled.","metadata":{"_uuid":"4aa77a41-83b7-4e0d-acde-664c9a9ed016","_cell_guid":"ead3c620-0e08-45ee-8af0-b4fe9fc1e91b","trusted":true,"collapsed":false,"papermill":{"duration":0.020799,"end_time":"2024-08-24T00:26:24.247197","exception":false,"start_time":"2024-08-24T00:26:24.226398","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# ðŸ” | Loss & Metric\n\nThis competition utilizes the Partial Area Under the Curve (pAUC) metric for evaluation. In Keras, a similar metric is ROC AUC, which can be used for approximate evaluation. For optimizing our model, binary cross entropy (BCE) loss from Keras will be used.\n\nIt's important to note that BCE loss is sensitive to class imbalance. To mitigate this issue, the class weights will be used which is computed earlier. These weights will prioritize updating the model weights more strongly for the minority class (positive class), thereby mitigating class imbalance.","metadata":{"_uuid":"c6aef30b-5d05-4793-b13b-829b47aced31","_cell_guid":"e58c40b3-ef6f-4166-b7fc-7a35dec7bc51","trusted":true,"collapsed":false,"papermill":{"duration":0.020738,"end_time":"2024-08-24T00:26:24.289039","exception":false,"start_time":"2024-08-24T00:26:24.268301","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# AUC\nauc = keras.metrics.AUC()\n\n# Loss\nloss = keras.losses.BinaryCrossentropy(label_smoothing=0.02)","metadata":{"_uuid":"6eada70b-080b-4498-89a8-583edc504b8f","_cell_guid":"49366f1d-732f-481c-84ab-2305c9561451","trusted":true,"collapsed":false,"papermill":{"duration":0.03872,"end_time":"2024-08-24T00:26:24.348670","exception":false,"start_time":"2024-08-24T00:26:24.309950","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:01:02.963725Z","iopub.execute_input":"2025-04-25T07:01:02.964369Z","iopub.status.idle":"2025-04-25T07:01:02.975709Z","shell.execute_reply.started":"2025-04-25T07:01:02.964329Z","shell.execute_reply":"2025-04-25T07:01:02.974994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ¤– | Modeling\n\nIn this notebook, we're using the `EfficientNetV2 B2` backbone from KerasCV's pretrained models to extract features from images. For processing tabular data, `Dense` layers are used. The final layer (head) is a `Dense` layer with a `sigmoid` activation function. The `sigmoid` activation is chosen because the target is binary; `softmax` would've been selected if the target was multiclass.\n\nTo explore other backbones, one can simply modify the `preset` in the `CFG` (config). A list of available pretrained backbones can be found on the [KerasCV website](https://keras.io/api/keras_cv/models/).\n\n> **Note**: Since the size of tabular features likely to change due to feature space processing,  `feature_space.get_encoded_features()` is used to determine the final size of the tabular feature vector for building the model.","metadata":{"_uuid":"9a30021e-a5de-47e3-8c0e-12769b6b34bc","_cell_guid":"1f95d3f1-09e3-4707-b744-30fd304d6194","trusted":true,"collapsed":false,"papermill":{"duration":0.020733,"end_time":"2024-08-24T00:26:24.390714","exception":false,"start_time":"2024-08-24T00:26:24.369981","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Define input layers\nimage_input = keras.Input(shape=(*CFG.image_size, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\ninp = {\"images\":image_input, \"features\":feat_input}\n\n# Branch for image input\nbackbone = keras_cv.models.EfficientNetV2Backbone.from_preset(CFG.preset)\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.Dropout(0.2)(x1)\n\n# Branch for tabular/feature input\nx2 = keras.layers.Dense(96, activation=\"selu\")(feat_input)\nx2 = keras.layers.Dense(128, activation=\"selu\")(x2)\nx2 = keras.layers.Dropout(0.1)(x2)\n\n# Concatenate both branches\nconcat = keras.layers.Concatenate()([x1, x2])\n\n# Output layer\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(concat)\n\n# Build model\nmodel = keras.models.Model(inp, out)\n\n# Compile the model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=loss,\n    metrics=[auc],\n)\n\n# Model Summary\nmodel.summary()","metadata":{"_uuid":"f9216305-ed6d-44a9-8599-a87cfa1daffd","_cell_guid":"1f54cfcc-e03f-49d1-a129-0b70f1a86735","trusted":true,"collapsed":false,"papermill":{"duration":11.391541,"end_time":"2024-08-24T00:26:35.803227","exception":false,"start_time":"2024-08-24T00:26:24.411686","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:01:02.976797Z","iopub.execute_input":"2025-04-25T07:01:02.977121Z","iopub.status.idle":"2025-04-25T07:01:10.165411Z","shell.execute_reply.started":"2025-04-25T07:01:02.977086Z","shell.execute_reply":"2025-04-25T07:01:10.164533Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot Model\n\nAs our model is multi-input, it is difficult to understand what is going on inside the architecture. That is where `plot_model` from **Keras** can be very handy. Overall architecture is shown, making it easier to design or recheck our architecture.","metadata":{"_uuid":"a57705b3-aa29-43c1-b026-8f3ca7f2bf7e","_cell_guid":"287fd12b-6191-471c-8e7c-3fd35d1be6b7","trusted":true,"collapsed":false,"papermill":{"duration":0.026338,"end_time":"2024-08-24T00:26:35.863410","exception":false,"start_time":"2024-08-24T00:26:35.837072","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, dpi=60)","metadata":{"_uuid":"fdb67e98-5d65-41d1-be09-04b97601ffa5","_cell_guid":"7fadf82b-4418-46d0-930a-b4707e83b092","trusted":true,"collapsed":false,"_kg_hide-input":false,"papermill":{"duration":0.287676,"end_time":"2024-08-24T00:26:36.173049","exception":false,"start_time":"2024-08-24T00:26:35.885373","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:01:10.166961Z","iopub.execute_input":"2025-04-25T07:01:10.167693Z","iopub.status.idle":"2025-04-25T07:01:10.409638Z","shell.execute_reply.started":"2025-04-25T07:01:10.167651Z","shell.execute_reply":"2025-04-25T07:01:10.408696Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# âš“ | LR Schedule\n\nA well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation.","metadata":{"_uuid":"6bebdbc8-a53b-42fc-ae44-45bd60b8515b","_cell_guid":"9acff6e1-6acf-44e7-accb-dcdcee61d26e","trusted":true,"collapsed":false,"papermill":{"duration":0.024017,"end_time":"2024-08-24T00:26:36.222948","exception":false,"start_time":"2024-08-24T00:26:36.198931","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import math\n\ndef get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n    lr_start, lr_max, lr_min = 2.5e-5, 5e-6 * batch_size, 0.8e-5\n    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n\n    def lrfn(epoch):  # Learning rate update function\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n        elif mode == 'cos':\n            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n        return lr\n\n    if plot:  # Plot lr curve if plot is True\n        plt.figure(figsize=(10, 5))\n        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('lr')\n        \n        plt.title('LR Scheduler')\n        plt.show()\n\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback","metadata":{"_uuid":"fe62899e-4c43-4fd9-af3c-f85785b176de","_cell_guid":"f0b0edc1-db06-4a48-86a1-f09d14625b92","trusted":true,"collapsed":false,"_kg_hide-input":true,"papermill":{"duration":0.037778,"end_time":"2024-08-24T00:26:36.285399","exception":false,"start_time":"2024-08-24T00:26:36.247621","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:01:13.271354Z","iopub.execute_input":"2025-04-25T07:01:13.272010Z","iopub.status.idle":"2025-04-25T07:01:13.280751Z","shell.execute_reply.started":"2025-04-25T07:01:13.271975Z","shell.execute_reply":"2025-04-25T07:01:13.279760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs, targets = next(iter(training_ds))\nimages = inputs[\"images\"]\nnum_images, NUMERIC_COLUMNS = 8, 4\n\nplt.figure(figsize=(4 * NUMERIC_COLUMNS, num_images // NUMERIC_COLUMNS * 4))\nfor i, (image, target) in enumerate(zip(images[:num_images], targets[:num_images])):\n    plt.subplot(num_images // NUMERIC_COLUMNS, NUMERIC_COLUMNS, i + 1)\n    image = image.numpy().astype(\"float32\")\n    target= target.numpy().astype(\"int32\")[0]\n    \n    image = (image - image.min()) / (image.max() + 1e-4)\n\n    plt.imshow(image)\n    plt.title(f\"Target: {target}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"7ba9a341-71f5-4667-85c9-829663b8942f","_cell_guid":"b722935d-7277-48b9-a774-72ff0c3ba058","trusted":true,"collapsed":false,"_kg_hide-input":true,"papermill":{"duration":3.396047,"end_time":"2024-08-24T00:26:39.705391","exception":false,"start_time":"2024-08-24T00:26:36.309344","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:01:15.065803Z","iopub.execute_input":"2025-04-25T07:01:15.066145Z","iopub.status.idle":"2025-04-25T07:01:17.143213Z","shell.execute_reply.started":"2025-04-25T07:01:15.066114Z","shell.execute_reply":"2025-04-25T07:01:17.142198Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr_cb = get_lr_callback(CFG.batch_size, mode=\"exp\", plot=True)","metadata":{"_uuid":"b9afe1d9-0aac-4d85-8fdc-b03740c19c6e","_cell_guid":"9d49fae6-931f-4d7a-945f-35471a758851","trusted":true,"collapsed":false,"papermill":{"duration":0.248004,"end_time":"2024-08-24T00:26:39.996218","exception":false,"start_time":"2024-08-24T00:26:39.748214","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:01:18.155698Z","iopub.execute_input":"2025-04-25T07:01:18.156028Z","iopub.status.idle":"2025-04-25T07:01:18.364703Z","shell.execute_reply.started":"2025-04-25T07:01:18.155999Z","shell.execute_reply":"2025-04-25T07:01:18.363792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ’¾ | Model Checkpoint\n\nThe `ModelCheckpoint` callback is used to save the model during training. It monitors the performance of the model on the validation data and saves the model with the best performance based on a specified metric.\n\nFollowing setup ensures that the model with the highest validation AUC (`val_auc`) during training is saved to the file `best_model.keras`. Only the best model is saved, preventing overwriting with worse-performing models.","metadata":{"_uuid":"44f33e06-2594-4a56-853c-7a8619410b13","_cell_guid":"4afe3c9f-edf9-4532-87ad-7b75bd79da83","trusted":true,"collapsed":false,"papermill":{"duration":0.044576,"end_time":"2024-08-24T00:26:40.084677","exception":false,"start_time":"2024-08-24T00:26:40.040101","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"ckpt_cb = keras.callbacks.ModelCheckpoint(\n    \"best_model.keras\",   # Filepath where the model will be saved.\n    monitor=\"val_auc\",    # Metric to monitor (validation AUC in this case).\n    save_best_only=True,  # Save only the model with the best performance.\n    save_weights_only=False,  # Save the entire model (not just the weights).\n    mode=\"max\",           # The model with the maximum 'val_auc' will be saved.\n)","metadata":{"_uuid":"d3e11740-2424-4148-a050-1b15d248832a","_cell_guid":"c8923063-fe08-4f87-8f2e-71bd13e7918c","trusted":true,"collapsed":false,"papermill":{"duration":0.052634,"end_time":"2024-08-24T00:26:40.182576","exception":false,"start_time":"2024-08-24T00:26:40.129942","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:01:22.321928Z","iopub.execute_input":"2025-04-25T07:01:22.322730Z","iopub.status.idle":"2025-04-25T07:01:22.326851Z","shell.execute_reply.started":"2025-04-25T07:01:22.322698Z","shell.execute_reply":"2025-04-25T07:01:22.326060Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸš‚ | Training\n\nFollwing cell will train the prepared model. Notice that, `class_weight` is used in the training.","metadata":{"_uuid":"f6f96ec3-e6f1-43ef-8194-f8e43422dde8","_cell_guid":"24210aaf-60c7-4b1e-ae7f-b28432228cb1","trusted":true,"collapsed":false,"papermill":{"duration":0.043497,"end_time":"2024-08-24T00:26:40.269999","exception":false,"start_time":"2024-08-24T00:26:40.226502","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"history = model.fit(\n    training_ds,\n    epochs=CFG.epochs,\n    callbacks=[lr_cb, ckpt_cb],\n    validation_data=validation_ds,\n    verbose=CFG.verbose,\n    class_weight=class_weights,\n)","metadata":{"_uuid":"9efbaeb4-44a1-4f6e-b74f-71dc9fc8762b","_cell_guid":"d084b4e6-9e2f-4d5d-983d-81da580db61a","trusted":true,"collapsed":false,"papermill":{"duration":388.946941,"end_time":"2024-08-24T00:33:09.261054","exception":false,"start_time":"2024-08-24T00:26:40.314113","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:01:26.232497Z","iopub.execute_input":"2025-04-25T07:01:26.233127Z","iopub.status.idle":"2025-04-25T07:08:28.765204Z","shell.execute_reply.started":"2025-04-25T07:01:26.233096Z","shell.execute_reply":"2025-04-25T07:08:28.764332Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize AUC with Epochs","metadata":{"_uuid":"5dafe427-aea7-4ded-ada9-8459539dc101","_cell_guid":"d229fffb-0e9a-445c-bed0-15251ec14532","trusted":true,"collapsed":false,"papermill":{"duration":0.05758,"end_time":"2024-08-24T00:33:09.376501","exception":false,"start_time":"2024-08-24T00:33:09.318921","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Extract AUC and validation AUC from history\nauc = history.history['auc']\nval_auc = history.history['val_auc']\nepochs = range(1, len(auc) + 1)\n\n# Find the epoch with the maximum val_auc\nmax_val_auc_epoch = np.argmax(val_auc)\nmax_val_auc = val_auc[max_val_auc_epoch]\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, auc, 'o-', label='Training AUC', markersize=5, color='tab:blue')\nplt.plot(epochs, val_auc, 's-', label='Validation AUC', markersize=5, color='tab:orange')\n\n# Highlight the max val_auc\nplt.scatter(max_val_auc_epoch + 1, max_val_auc, color='red', s=100, label=f'Max Val AUC: {max_val_auc:.4f}')\nplt.annotate(f'Max Val AUC: {max_val_auc:.4f}', \n             xy=(max_val_auc_epoch + 1, max_val_auc), \n             xytext=(max_val_auc_epoch + 1 + 0.5, max_val_auc - 0.05),\n             arrowprops=dict(facecolor='black', arrowstyle=\"->\"),\n             fontsize=12,\n             color='tab:red')\n\n# Enhancing the plot\nplt.title('AUC over Epochs', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('AUC', fontsize=12)\nplt.legend(loc='lower right', fontsize=12)\nplt.grid(True)\nplt.xticks(epochs)\n\n# Show the plot\nplt.show()","metadata":{"_uuid":"6c1cea1a-eb21-402c-8db3-81c16660ee2f","_cell_guid":"e7bba8bc-b7ab-45e8-bcce-bac05d504854","trusted":true,"collapsed":false,"_kg_hide-input":true,"papermill":{"duration":0.438174,"end_time":"2024-08-24T00:33:09.870545","exception":false,"start_time":"2024-08-24T00:33:09.432371","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:08:28.767420Z","iopub.execute_input":"2025-04-25T07:08:28.767683Z","iopub.status.idle":"2025-04-25T07:08:29.079468Z","shell.execute_reply.started":"2025-04-25T07:08:28.767658Z","shell.execute_reply":"2025-04-25T07:08:29.078670Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“‹ | Result","metadata":{"_uuid":"ec507b29-e309-4776-8729-24e70c99d579","_cell_guid":"7d6a66e9-d0f2-475d-ab5c-4b5397e5d2c1","trusted":true,"collapsed":false,"papermill":{"duration":0.057075,"end_time":"2024-08-24T00:33:09.985784","exception":false,"start_time":"2024-08-24T00:33:09.928709","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Best Result\nbest_score = max(history.history['val_auc'])\nbest_epoch = np.argmax(history.history['val_auc']) + 1\nprint(\"#\" * 10 + \" Result \" + \"#\" * 10)\nprint(f\"Best AUC: {best_score:.5f}\")\nprint(f\"Best Epoch: {best_epoch}\")\nprint(\"#\" * 28)","metadata":{"_uuid":"10719673-948d-41e5-bc3e-cb3f32ce5042","_cell_guid":"0f651f23-f221-4326-a88a-f163bf9868ab","trusted":true,"collapsed":false,"papermill":{"duration":0.069471,"end_time":"2024-08-24T00:33:10.113463","exception":false,"start_time":"2024-08-24T00:33:10.043992","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:08:29.080688Z","iopub.execute_input":"2025-04-25T07:08:29.081022Z","iopub.status.idle":"2025-04-25T07:08:29.086746Z","shell.execute_reply.started":"2025-04-25T07:08:29.080984Z","shell.execute_reply":"2025-04-25T07:08:29.085899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ§ª | Prediction","metadata":{"_uuid":"c464cdb8-4021-4e0e-ae9e-a3373226f1d3","_cell_guid":"2999988c-affb-419e-b47d-f27475053acd","trusted":true,"collapsed":false,"papermill":{"duration":0.058198,"end_time":"2024-08-24T00:33:10.229522","exception":false,"start_time":"2024-08-24T00:33:10.171324","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Load Best Model","metadata":{"_uuid":"ad65c549-032f-42fe-a083-644e65beb825","_cell_guid":"45152dff-b2d2-421e-afdf-05c9d9675658","trusted":true,"collapsed":false,"papermill":{"duration":0.05779,"end_time":"2024-08-24T00:33:10.345958","exception":false,"start_time":"2024-08-24T00:33:10.288168","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model.load_weights(\"best_model.keras\")","metadata":{"_uuid":"65d42a32-442c-45c9-80bc-1e234af78cec","_cell_guid":"33067714-a8e8-438d-8339-5bf908574340","trusted":true,"collapsed":false,"papermill":{"duration":7.831032,"end_time":"2024-08-24T00:33:18.233895","exception":false,"start_time":"2024-08-24T00:33:10.402863","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-19T04:01:57.042402Z","iopub.execute_input":"2025-03-19T04:01:57.042711Z","iopub.status.idle":"2025-03-19T04:02:05.988840Z","shell.execute_reply.started":"2025-03-19T04:01:57.042683Z","shell.execute_reply":"2025-03-19T04:02:05.987781Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build Testing Dataset\n\nDon't forget to normalize for the testing data as well.","metadata":{"_uuid":"8bd2f506-cc89-4168-a5bd-fed13ac58820","_cell_guid":"892aef78-8565-458b-b8cb-04e80632ee3c","trusted":true,"collapsed":false,"papermill":{"duration":0.05811,"end_time":"2024-08-24T00:33:18.350427","exception":false,"start_time":"2024-08-24T00:33:18.292317","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Testing\nprint(\"# Testing:\")\ntesting_features = dict(testing_df[FEAT_COLS])\ntesting_ids = testing_df.isic_id.values\ntesting_ds = build_dataset(testing_ids, testing_hdf5,\n                        testing_features, batch_size=CFG.batch_size,\n                         shuffle=False, augment=False, cache=False)\n# Apply feature space processing\ntesting_ds = testing_ds.map(\n    lambda x: {\"images\": x[\"images\"],\n               \"features\": feature_space(x[\"features\"])}, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"_uuid":"c5d0c024-9a02-43d1-829f-7d63f385c662","_cell_guid":"79fa6822-3460-4e41-9722-160de2419166","trusted":true,"collapsed":false,"papermill":{"duration":0.272864,"end_time":"2024-08-24T00:33:18.682051","exception":false,"start_time":"2024-08-24T00:33:18.409187","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-19T04:02:05.990572Z","iopub.execute_input":"2025-03-19T04:02:05.990843Z","iopub.status.idle":"2025-03-19T04:02:06.189371Z","shell.execute_reply.started":"2025-03-19T04:02:05.990816Z","shell.execute_reply":"2025-03-19T04:02:06.188475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{"_uuid":"25168073-7fa3-48a8-91f3-94f17177d814","_cell_guid":"15a4fdc8-91e2-45d7-a57f-1483cbc9e15e","trusted":true,"collapsed":false,"papermill":{"duration":0.058486,"end_time":"2024-08-24T00:33:18.799793","exception":false,"start_time":"2024-08-24T00:33:18.741307","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"preds = model.predict(testing_ds).squeeze()","metadata":{"_uuid":"2ce947f1-26f3-4268-9d86-07556cf4b1ca","_cell_guid":"11e9346e-860a-4c73-8ab0-2e42f096b1bc","trusted":true,"collapsed":false,"papermill":{"duration":7.326659,"end_time":"2024-08-24T00:33:26.185751","exception":false,"start_time":"2024-08-24T00:33:18.859092","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-19T04:02:06.190682Z","iopub.execute_input":"2025-03-19T04:02:06.191010Z","iopub.status.idle":"2025-03-19T04:02:14.096190Z","shell.execute_reply.started":"2025-03-19T04:02:06.190978Z","shell.execute_reply":"2025-03-19T04:02:14.095231Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check Prediction","metadata":{"_uuid":"1c4a25f7-51e1-47dd-8807-968ccfe1aadb","_cell_guid":"c11c49ee-98dd-440b-a42e-115d8f113712","trusted":true,"collapsed":false,"papermill":{"duration":0.058354,"end_time":"2024-08-24T00:33:26.303388","exception":false,"start_time":"2024-08-24T00:33:26.245034","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"inputs = next(iter(testing_ds))\nimages = inputs[\"images\"]\n\n# Plotting\nplt.figure(figsize=(10, 4))\n\nfor i in range(3):\n    plt.subplot(1, 3, i+1)  # 1 row, 3 columns, i+1th subplot\n    plt.imshow(images[i])  # Show image\n    plt.title(f'Prediction: {preds[i]:.2f}')  # Set title with prediction\n    plt.axis('off')  # Hide axis\n\nplt.suptitle('Model Predictions on Testing Images', fontsize=16)\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"10e0df88-12f8-45b0-8b1b-d20038cae84c","_cell_guid":"fa0612d4-907f-450a-b8d9-e0436a158efd","trusted":true,"collapsed":false,"papermill":{"duration":0.591497,"end_time":"2024-08-24T00:33:26.952319","exception":false,"start_time":"2024-08-24T00:33:26.360822","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:10:54.790091Z","iopub.execute_input":"2025-04-25T07:10:54.790453Z","iopub.status.idle":"2025-04-25T07:10:54.822324Z","shell.execute_reply.started":"2025-04-25T07:10:54.790421Z","shell.execute_reply":"2025-04-25T07:10:54.821295Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DenseNet121","metadata":{"_uuid":"95e62948-76a3-49ef-bdf8-3c3987142a0b","_cell_guid":"093c7ba8-f764-4e0a-89d6-dc2b9fd2af85","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.metrics import Precision, Recall, AUC, Accuracy  # Import the necessary metrics\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\ninp = {\"images\": image_input, \"features\": feat_input}\n\n# Branch for image input using EfficientNet backbone\nbackbone = EfficientNetB0(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)  # Adding Batch Normalization\n\n# Branch for tabular/feature input\nx2 = keras.layers.Dense(128, activation=\"relu\")(feat_input)  # Increased units and changed activation\nx2 = keras.layers.Dense(256, activation=\"relu\")(x2)  # Increased units\nx2 = keras.layers.BatchNormalization()(x2)  # Adding Batch Normalization\n\n# Concatenate both branches\nconcat = keras.layers.Concatenate()([x1, x2])\n\n# Output layer for binary classification (benign vs malignant)\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(concat)\n\n# Build the multi-modal model\nmodel = keras.models.Model(inputs=inp, outputs=out)\n\n# Compile the model\nauc = AUC(name=\"auc\")\nprecision = Precision(name=\"precision\")  # Define precision metric\nrecall = Recall(name=\"recall\")  # Define recall metric\naccuracy = Accuracy(name=\"accuracy\")  # Define accuracy metric\nloss = keras.losses.BinaryCrossentropy(from_logits=False)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=loss,\n    metrics=[auc, precision, recall],  # Add precision, recall, and accuracy here\n)\n\n# Model summary\nmodel.summary()\n\n# Training the model\nhistory = model.fit(\n    training_ds,\n    epochs=30,\n    callbacks=[lr_cb, ckpt_cb],\n    validation_data=validation_ds,\n    verbose=CFG.verbose,\n    class_weight=class_weights,\n)","metadata":{"_uuid":"1e58c47b-6695-491f-8339-dd7fe905108b","_cell_guid":"93bd0201-f4e7-4b29-ac15-80e854223476","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Find the epoch with the best validation AUC\nbest_val_auc = history.history['val_auc'][best_epoch]  # Best AUC at that epoch\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_precision = history.history['precision'][best_epoch]\nbest_recall = history.history['recall'][best_epoch]\n\n# Step 3: Print the results\nprint(f\"Best Precision: {best_precision:.5f}\")\nprint(f\"Best Recall: {best_recall:.5f}\")\nprint(f\"Best AUC: {best_val_auc:.5f}\")","metadata":{"_uuid":"8b9718ec-494e-4599-be45-3eff94bccd1b","_cell_guid":"658eb9c6-9020-4b14-bc11-98b8ea99ef5b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ResNet50","metadata":{"_uuid":"8ff5108f-1b77-4eaf-83dc-52edd1355587","_cell_guid":"5cb98aac-8901-4229-a395-176ebbd10b32","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.metrics import Precision, Recall, AUC\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\ninp = {\"images\": image_input, \"features\": feat_input}\n\n# Branch for image input using ResNet50\nbackbone = ResNet50(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)\nx1 = keras.layers.Dropout(0.3)(x1)\n\n# Branch for tabular/feature input\nx2 = keras.layers.Dense(128, activation=\"relu\")(feat_input)\nx2 = keras.layers.BatchNormalization()(x2)\nx2 = keras.layers.Dense(64, activation=\"relu\")(x2)\nx2 = keras.layers.Dropout(0.2)(x2)\n\n# Concatenate both branches\nconcat = keras.layers.Concatenate()([x1, x2])\n\n# Additional Dense layers after concatenation for deeper learning\nx = keras.layers.Dense(64, activation=\"relu\")(concat)\nx = keras.layers.BatchNormalization()(x)\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(x)\n\n# Build the multi-modal model\nmodel = keras.models.Model(inputs=inp, outputs=out)\n\n# Compile the model with additional metrics (precision, recall, AUC)\nauc = AUC(name=\"auc\")\nprecision = Precision(name=\"precision\")\nrecall = Recall(name=\"recall\")\naccuracy = keras.metrics.BinaryAccuracy(name=\"accuracy\")\n\nloss = keras.losses.BinaryCrossentropy(from_logits=False)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=loss,\n    metrics=[auc, precision, recall],  # Add precision, recall, and accuracy here\n)\n\n# Model summary\nmodel.summary()\n\n# Training the model\nhistory = model.fit(\n    training_ds,\n    epochs=30,\n    callbacks=[lr_cb, ckpt_cb],\n    validation_data=validation_ds,\n    verbose=CFG.verbose,\n    class_weight=class_weights,\n)","metadata":{"_uuid":"48c92a88-c675-4b1d-94ca-dc812f879878","_cell_guid":"aa774435-2235-4e53-adb4-3cceb3915a9c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Find the epoch with the best validation AUC\nbest_val_auc = history.history['val_auc'][best_epoch]  # Best AUC at that epoch\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_precision = history.history['precision'][best_epoch]\nbest_recall = history.history['recall'][best_epoch]\n\n# Step 3: Print the results\nprint(f\"Best Precision: {best_precision:.5f}\")\nprint(f\"Best Recall: {best_recall:.5f}\")\nprint(f\"Best AUC: {best_val_auc:.5f}\")","metadata":{"_uuid":"ddffa9fa-6d02-446f-9c2a-682acc04768e","_cell_guid":"8dc77947-23ce-446f-8b3c-b33d44751be8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## InceptionV3","metadata":{"_uuid":"96abf855-3ba4-42fc-ac82-dcabcf563e63","_cell_guid":"59c9d973-456f-44b6-8b3d-f0454135e445","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import InceptionV3\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\ninp = {\"images\": image_input, \"features\": feat_input}\n\n# Branch for image input using InceptionV3\nbackbone = InceptionV3(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)  # Added BatchNormalization\nx1 = keras.layers.Dropout(0.3)(x1)  # Increased Dropout rate\n\n# Branch for tabular/feature input\nx2 = keras.layers.Dense(128, activation=\"relu\")(feat_input)  # Increased the first layer size\nx2 = keras.layers.BatchNormalization()(x2)  # Added BatchNormalization\nx2 = keras.layers.Dense(64, activation=\"relu\")(x2)  # Changed second layer size\nx2 = keras.layers.Dropout(0.2)(x2)  # Modified Dropout rate\n\n# Concatenate both branches\nconcat = keras.layers.Concatenate()([x1, x2])\n\n# Additional Dense layers after concatenation for deeper learning\nx = keras.layers.Dense(64, activation=\"relu\")(concat)  # New dense layer\nx = keras.layers.BatchNormalization()(x)  # Added BatchNormalization\nx = keras.layers.Dropout(0.3)(x)  # Increased Dropout rate\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(x)\n\n# Build the multi-modal model\nmodel = keras.models.Model(inputs=inp, outputs=out)\n\n# Compile the model\nauc = keras.metrics.AUC(name=\"auc\")\nloss = keras.losses.BinaryCrossentropy(from_logits=False)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=loss,\n    metrics=[auc, precision, recall],  # Add precision, recall, and accuracy here\n)\n\n# Model summary\nmodel.summary()\n\n# Training the model\nhistory = model.fit(\n    training_ds,\n    epochs=30,\n    callbacks=[lr_cb, ckpt_cb],\n    validation_data=validation_ds,\n    verbose=CFG.verbose,\n    class_weight=class_weights,\n)","metadata":{"_uuid":"e1189fec-eb13-4612-948e-92fd13cbd1a1","_cell_guid":"64e86db0-deeb-4d38-a504-b78c0605c357","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Find the epoch with the best validation AUC\nbest_val_auc = history.history['val_auc'][best_epoch]  # Best AUC at that epoch\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_precision = history.history['precision'][best_epoch]\nbest_recall = history.history['recall'][best_epoch]\n\n# Step 3: Print the results\nprint(f\"Best Precision: {best_precision:.5f}\")\nprint(f\"Best Recall: {best_recall:.5f}\")\nprint(f\"Best AUC: {best_val_auc:.5f}\")","metadata":{"_uuid":"6c438dab-4ab3-4845-ab60-d25c37c912c2","_cell_guid":"b5c8deec-6727-498f-9080-076758b71323","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hierarchical Multi-Scale Attention Fusion Network","metadata":{"_uuid":"c743f1a0-6f6a-450d-8bc3-a94565e7c402","_cell_guid":"3fc607d3-006c-4699-b6b8-b7c689c5b67c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Updated one","metadata":{"_uuid":"b86211e9-5b6c-4d23-870b-ce66e1ac5333","_cell_guid":"7ef4c61f-b739-4f5b-862f-2f2a7693fdce","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.metrics import AUC, Precision, Recall, Accuracy\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\n\n# ----------------- Image Branch -----------------\n# EfficientNet Backbone with Pretrained Weights\nbackbone = EfficientNetB0(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)\nx1 = keras.layers.Dropout(0.5)(x1)  # Increased dropout in the image branch\n\n# First latent projection for image embeddings\nx1_latent = keras.layers.Dense(512, activation=\"relu\", name=\"image_latent_projection\")(x1)\nx1_latent = keras.layers.BatchNormalization()(x1_latent)\n\n# ----------------- Feature Branch -----------------\n# Tabular Data Feature Processing with Increased Dense Layer Sizes\nx2 = keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(feat_input)\nx2 = keras.layers.BatchNormalization()(x2)\nx2 = keras.layers.Dropout(0.4)(x2)  # Increased dropout in the feature branch\n\n# First latent projection for feature embeddings\nx2_latent = keras.layers.Dense(512, activation=\"relu\", name=\"feature_latent_projection\")(x2)\nx2_latent = keras.layers.BatchNormalization()(x2_latent)\n\n# ----------------- Latent Space Alignment -----------------\n# Discriminator for Alignment with Increased Network Capacity\ndef make_discriminator():\n    d_input = keras.Input(shape=(512,))\n    d_x = keras.layers.Dense(256, activation=\"relu\")(d_input)\n    d_x = keras.layers.BatchNormalization()(d_x)\n    d_x = keras.layers.Dense(128, activation=\"relu\")(d_x)\n    d_x = keras.layers.BatchNormalization()(d_x)\n    d_x = keras.layers.Dense(64, activation=\"relu\")(d_x)\n    d_output = keras.layers.Dense(1, activation=\"sigmoid\", name=\"discriminator_output\")(d_x)\n    return keras.models.Model(inputs=d_input, outputs=d_output, name=\"Discriminator\")\n\ndiscriminator = make_discriminator()\n\n# Latent space alignment\nd_image = discriminator(x1_latent)\nd_feature = discriminator(x2_latent)\n\n# Loss for discriminator alignment\nadversarial_loss = keras.losses.BinaryCrossentropy(from_logits=False)\n\n# ----------------- Feature Aggregation -----------------\n# Concatenate Latent Spaces\nconcat = keras.layers.Concatenate()([x1_latent, x2_latent])\n\n# Hierarchical Feature Aggregation with Larger Dense Layer\nagg = keras.layers.Dense(2048, activation=\"relu\", name=\"aggregated_features\")(concat)\nagg = keras.layers.BatchNormalization()(agg)\nagg = keras.layers.Dropout(0.5)(agg)  # Increased dropout\n\n# ----------------- Output Layer -----------------\n# Final Binary Classification\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\", name=\"output\")(agg)\n\n# Build the MedBlendNet Model\nmedblendnet = keras.models.Model(inputs={\"images\": image_input, \"features\": feat_input}, outputs=out)\n\n# ----------------- Compile the Model -----------------\n# Early stopping and learning rate scheduler for better performance\nearly_stopping_cb = EarlyStopping(monitor=\"val_auc\", patience=10, restore_best_weights=True, verbose=1, mode='max')\nlr_scheduler = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, verbose=1, mode='max')\n\n# Compile the model\nmedblendnet.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=\"binary_crossentropy\",\n    metrics=[AUC(), Precision(), Recall()]  # Correctly instantiate the metrics\n)\n\n# Model summary\nmedblendnet.summary()\n# ----------------- Train the Model -----------------\nhistory = medblendnet.fit(\n    training_ds,\n    epochs=10,\n    callbacks=[lr_scheduler, early_stopping_cb, ckpt_cb],  # Add checkpoints if needed\n    validation_data=validation_ds,\n    verbose=CFG.verbose,\n    class_weight=class_weights,\n)","metadata":{"_uuid":"889c5690-4b67-46c8-9462-61f1b5659ea8","_cell_guid":"49e4e442-96d0-44f0-aa26-90896c554686","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-25T07:16:29.171150Z","iopub.execute_input":"2025-04-25T07:16:29.172243Z","iopub.status.idle":"2025-04-25T07:22:04.413492Z","shell.execute_reply.started":"2025-04-25T07:16:29.172181Z","shell.execute_reply":"2025-04-25T07:22:04.412582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import numpy as np\n\n# Step 1: Find the epoch with the best validation AUC\nbest_epoch = np.argmax(history.history['val_auc_2'])  # Index of the epoch with the highest validation AUC\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_precision = history.history['precision_1'][best_epoch]\nbest_recall = history.history['recall_1'][best_epoch]\nbest_val_auc = history.history['val_auc_2'][best_epoch]\n\n# Step 3: Print the results\nprint(f\"Best Precision at epoch {best_epoch + 1}: {best_precision:.5f}\")\nprint(f\"Best Recall at epoch {best_epoch + 1}: {best_recall:.5f}\")\nprint(f\"Best Validation AUC at epoch {best_epoch + 1}: {best_val_auc:.5f}\")","metadata":{"_uuid":"c582db63-7ad5-4acb-92e6-42d3be4fd2f4","_cell_guid":"6fbe0cb3-f8c3-4bc9-970c-e6db2e131afd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport time\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n\ndef benchmark_model(model, model_name=\"Model\", img_shape=(128, 128, 3), tabular_shape=(10,), runs=100):\n    print(f\"\\nðŸ“Š Benchmarking: {model_name}\")\n    print(\"-\" * 50)\n\n    # ------------------ Parameters ------------------\n    total_params = model.count_params()\n    print(f\"ðŸ”¢ Total Parameters: {total_params / 1e6:.2f} Million\")\n\n    # ------------------ FLOPs ------------------\n    try:\n        func = tf.function(lambda images, features: model({\"images\": images, \"features\": features}))\n        concrete_func = func.get_concrete_function(\n            images=tf.TensorSpec([1, *img_shape], tf.float32),\n            features=tf.TensorSpec([1, *tabular_shape], tf.float32)\n        )\n\n        frozen_func = convert_variables_to_constants_v2(concrete_func)\n        graph_def = frozen_func.graph.as_graph_def()\n\n        with tf.Graph().as_default() as graph:\n            tf.import_graph_def(graph_def, name='')\n            run_meta = tf.compat.v1.RunMetadata()\n            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n            flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, options=opts)\n            print(f\"âš™ï¸ FLOPs: {flops.total_float_ops / 1e9:.4f} GFLOPs\")\n    except Exception as e:\n        print(f\"âš ï¸ FLOPs calculation failed: {e}\")\n\n    # ------------------ Inference Time ------------------\n    try:\n        img_input = np.random.rand(1, *img_shape).astype(np.float32)\n        tab_input = np.random.rand(1, *tabular_shape).astype(np.float32)\n\n        # Warm-up\n        for _ in range(5):\n            _ = model.predict({\"images\": img_input, \"features\": tab_input}, verbose=0)\n\n        # Timed runs\n        start = time.time()\n        for _ in range(runs):\n            _ = model.predict({\"images\": img_input, \"features\": tab_input}, verbose=0)\n        end = time.time()\n\n        avg_time = (end - start) / runs\n        print(f\"â±ï¸ Avg Inference Time (over {runs} runs): {avg_time:.6f} seconds\")\n    except Exception as e:\n        print(f\"âš ï¸ Inference timing failed: {e}\")\n\n    print(\"-\" * 50)","metadata":{"_uuid":"b28c74d4-7dfc-43ba-abc4-d9db93c51356","_cell_guid":"1e2fd9b6-702e-4c46-bf07-d0f06b25f83e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-25T07:24:04.319476Z","iopub.execute_input":"2025-04-25T07:24:04.319841Z","iopub.status.idle":"2025-04-25T07:24:04.330533Z","shell.execute_reply.started":"2025-04-25T07:24:04.319808Z","shell.execute_reply":"2025-04-25T07:24:04.329767Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Your MedBlendNet\nbenchmark_model(medblendnet, model_name=\"MedBlendNet\", img_shape=(128,128,3), tabular_shape=(feature_space.get_encoded_features().shape[1],))","metadata":{"_uuid":"3efb0027-8e9e-4280-b5bb-dcd0a573070e","_cell_guid":"d28d97dc-218b-4a57-93df-81088203703e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-25T07:24:07.753332Z","iopub.execute_input":"2025-04-25T07:24:07.754153Z","iopub.status.idle":"2025-04-25T07:24:22.835375Z","shell.execute_reply.started":"2025-04-25T07:24:07.754120Z","shell.execute_reply":"2025-04-25T07:24:22.834488Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CMMANN","metadata":{"_uuid":"6a4620c9-d5a8-444a-ae36-56a5c5f0d16e","_cell_guid":"665c530e-187a-4632-a6c0-8e475452c591","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.applications import EfficientNetB4\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n\n# Memory-Augmented Network\nclass MemoryAugmentedNetwork(keras.layers.Layer):\n    def __init__(self, memory_size=512, memory_dim=256, temperature=0.1, **kwargs):\n        super(MemoryAugmentedNetwork, self).__init__(**kwargs)\n        self.memory_size = memory_size\n        self.memory_dim = memory_dim\n        self.temperature = temperature\n        self.memory = self.add_weight(\n            name=\"memory\",\n            shape=(memory_size, memory_dim),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n        )\n        \n    def call(self, inputs):\n        # Scaled dot-product attention with temperature\n        similarity = tf.matmul(inputs, self.memory, transpose_b=True) / self.temperature\n        attention_weights = tf.nn.softmax(similarity, axis=-1)\n        memory_read = tf.matmul(attention_weights, self.memory)\n        # Residual connection\n        return memory_read + inputs\n\n# Image and Tabular Data Inputs\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(71,), name=\"features\")\n\n# ----------------- Image Branch -----------------\n# Using EfficientNetB4 for better feature extraction without pre-trained weights\nbackbone = EfficientNetB4(weights=None, include_top=False, input_shape=(128, 128, 3))\n\n# Freeze early layers\nfor layer in backbone.layers[:100]:\n    layer.trainable = False\n\nx1 = backbone(image_input)\nx1 = layers.GlobalAveragePooling2D()(x1)\nx1 = layers.BatchNormalization()(x1)\nx1 = layers.Dense(512, activation=\"relu\")(x1)\nx1 = layers.Dropout(0.5)(x1)  # Increased dropout\n\n# Latent Representation for Image Features\nx1_latent = layers.Dense(512, activation=\"relu\", \n                        kernel_regularizer=regularizers.l2(1e-4),\n                        name=\"image_latent_projection\")(x1)\n\n# ----------------- Feature Branch -----------------\n# Enhanced Tabular Data Processing\nx2 = layers.Dense(256, activation=\"relu\")(feat_input)\nx2 = layers.BatchNormalization()(x2)\nx2 = layers.Dense(512, activation=\"relu\")(x2)\nx2 = layers.BatchNormalization()(x2)\nx2 = layers.Dropout(0.5)(x2)  # Increased dropout\n\n# Latent Representation for Tabular Features\nx2_latent = layers.Dense(512, activation=\"relu\",\n                        kernel_regularizer=regularizers.l2(1e-4),\n                        name=\"feature_latent_projection\")(x2)\n\n# ----------------- Memory-Augmented Module -----------------\nmemory_module = MemoryAugmentedNetwork(memory_size=512, memory_dim=256, temperature=0.1)\nx1_latent_projected = layers.Dense(256, activation=\"relu\")(x1_latent)\nx2_latent_projected = layers.Dense(256, activation=\"relu\")(x2_latent)\n\nx1_mem = memory_module(x1_latent_projected)\nx2_mem = memory_module(x2_latent_projected)\n\n# ----------------- Enhanced Contrastive Loss -----------------\ndef improved_contrastive_loss(y_true, y_pred, margin=1.5, alpha=0.6):\n    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    \n    # Enhanced contrastive component\n    pos_pair_distance = tf.reduce_sum(tf.square(y_pred - y_true), axis=-1)\n    neg_pair_distance = tf.maximum(margin - pos_pair_distance, 0.0)\n    \n    # Focal loss component\n    gamma = 2.0\n    focal_weight = tf.pow(1. - y_pred, gamma) * y_true + tf.pow(y_pred, gamma) * (1. - y_true)\n    focal = focal_weight * bce\n    \n    return alpha * (pos_pair_distance + neg_pair_distance) + (1 - alpha) * focal\n\n# ----------------- Feature Aggregation -----------------\nconcat = layers.Concatenate(axis=-1)([x1_mem, x2_mem])\n\n# Enhanced Cross-Modal Feature Aggregation\nagg = layers.Dense(1024, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(concat)\nagg = layers.BatchNormalization()(agg)\nagg = layers.Dropout(0.5)(agg)  # Increased dropout\nagg = layers.Dense(512, activation=\"relu\")(agg)\nagg = layers.BatchNormalization()(agg)\nagg = layers.Dropout(0.5)(agg)  # Increased dropout\n\n# ----------------- Output Layer -----------------\nout = layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(agg)\n\n# Build Model\ncmmann = keras.models.Model(inputs={\"images\": image_input, \"features\": feat_input}, outputs=out)\n\n# Compile with custom metrics\ncmmann.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n    loss=improved_contrastive_loss,\n    metrics=[AUC(), Precision(), Recall()]  # Correctly instantiate the metrics\n\n)\n\n# ----------------- Enhanced Callbacks -----------------\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_auc',\n    factor=0.4,  # Lower factor\n    patience=6,  # Adjusted patience\n    min_lr=1e-6,\n    mode='max'\n)\n\nearly_stopping = EarlyStopping(\n    monitor='val_auc',\n    patience=20,  # Increased patience\n    restore_best_weights=True,\n    mode='max'\n)\n\n# ----------------- Enhanced Data Augmentation -----------------\ntrain_datagen = ImageDataGenerator(\n    rotation_range=30,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    vertical_flip=False,\n    fill_mode='nearest',\n    brightness_range=[0.7, 1.3],\n    channel_shift_range=50.0\n)\nkeras.utils.plot_model(cmmann, show_shapes=True, show_layer_names=True, dpi=60)\n\n# ----------------- Training -----------------\nhistory = cmmann.fit(\n    training_ds,\n    epochs=20,  # Increased epochs\n    callbacks=[early_stopping, reduce_lr],\n    validation_data=validation_ds,\n    verbose=CFG.verbose,\n    class_weight=class_weights\n)","metadata":{"_uuid":"7ed95151-b023-4e1c-91b1-3394dd6bfbc2","_cell_guid":"19c5316d-7f85-44c6-91fa-79116ef23a39","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-25T07:31:45.832397Z","iopub.execute_input":"2025-04-25T07:31:45.832739Z","iopub.status.idle":"2025-04-25T07:48:08.014761Z","shell.execute_reply.started":"2025-04-25T07:31:45.832709Z","shell.execute_reply":"2025-04-25T07:48:08.014033Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import numpy as np\n\n# Step 1: Find the epoch with the best validation AUC\nbest_epoch = np.argmax(history.history['val_auc_3'])  # Index of the epoch with the highest validation AUC\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_precision = history.history['precision_2'][best_epoch]\nbest_recall = history.history['recall_2'][best_epoch]\nbest_val_auc = history.history['val_auc_3'][best_epoch]\n\n# Step 3: Print the results\nprint(f\"Best Precision at epoch {best_epoch + 1}: {best_precision:.5f}\")\nprint(f\"Best Recall at epoch {best_epoch + 1}: {best_recall:.5f}\")\nprint(f\"Best Validation AUC at epoch {best_epoch + 1}: {best_val_auc:.5f}\")","metadata":{"_uuid":"97a81828-c5da-4fbb-8387-73363ee6e88d","_cell_guid":"f65136c0-156e-4bdf-8026-510cb1a824c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Attention Model","metadata":{"_uuid":"5fd00de4-787a-4a1f-aa6b-04de987fe386","_cell_guid":"774043c9-c760-4fde-89dd-bd3a3dbc851e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers import Attention, Add, MultiHeadAttention, Dense, Flatten, BatchNormalization, Concatenate, Dropout, GlobalAveragePooling2D, Reshape\nfrom tensorflow.keras.metrics import Precision, Recall, AUC, Accuracy\nfrom tensorflow.keras import regularizers\n\n# Define input layers for images and tabular metadata\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(feature_space.get_encoded_features().shape[1],), name=\"features\")\n\n# EfficientNetB0 Backbone with Batch Normalization\nbackbone = EfficientNetB0(weights=None, include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = GlobalAveragePooling2D()(x1)\nx1 = BatchNormalization()(x1)  # Adding Batch Normalization\nx1 = Dropout(0.3)(x1)  # Dropout to prevent overfitting\n\n# Attention Mechanism\nx1_reshaped = Reshape((1, 1280))(x1)\nx1_attention = MultiHeadAttention(num_heads=8, key_dim=128)(x1_reshaped, x1_reshaped)\nx1 = Add()([x1_reshaped, x1_attention])\nx1 = BatchNormalization()(x1)  # Batch normalization after attention\nx1 = Flatten()(x1)\n\n# Tabular Branch with DenseNet-like connections and regularization\nx2 = Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(feat_input)\nx2 = Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(x2)\nx2 = BatchNormalization()(x2)\nx2 = Dropout(0.3)(x2)  # Dropout for regularization\n\n# Attention mechanism for tabular features\nx2_reshaped = Reshape((1, 256))(x2)\nx2_attention = MultiHeadAttention(num_heads=8, key_dim=128)(x2_reshaped, x2_reshaped)\nx2 = Add()([x2_reshaped, x2_attention])\nx2 = BatchNormalization()(x2)  # Batch normalization after attention\nx2 = Flatten()(x2)\n\n# Combine Image and Tabular Branches\nconcat = Concatenate()([x1, x2])\n\n# Output Layer\nout = Dense(1, activation=\"sigmoid\")(concat)\n\n# Build Model\nmodel = keras.models.Model(inputs=[image_input, feat_input], outputs=out)\n\n# Compile the model\nauc = AUC(name=\"auc\")\nprecision = Precision(name=\"precision\")\nrecall = Recall(name=\"recall\")\naccuracy = Accuracy(name=\"accuracy\")\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss='binary_crossentropy',\n    metrics=[AUC(), Precision(), Recall()]  # Correctly instantiate the metrics\n)\n\nkeras.utils.plot_model(model, show_shapes=True, show_layer_names=True, dpi=60)\n\nfrom tensorflow.keras.utils import plot_model\n\nplot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True, dpi=150)\nfrom IPython.display import FileLink\n\n# Display a download link\nFileLink('model_architecture.png')\n# Model summary","metadata":{"_uuid":"a716ab42-4e75-4fa4-b4d4-92c6cbfa4e45","_cell_guid":"a539de1d-10c9-4cc6-bfcd-0936f45d5827","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import numpy as np\n\n# Step 1: Find the epoch with the best validation AUC\nbest_epoch = np.argmax(history.history['val_auc_4'])  # Index of the epoch with the highest validation AUC\n\n# Step 2: Extract the corresponding metrics at that epoch\nbest_precision = history.history['precision_3'][best_epoch]\nbest_recall = history.history['recall_3'][best_epoch]\nbest_val_auc = history.history['val_auc_4'][best_epoch]\n\n# Step 3: Print the results\nprint(f\"Best Precision at epoch {best_epoch + 1}: {best_precision:.5f}\")\nprint(f\"Best Recall at epoch {best_epoch + 1}: {best_recall:.5f}\")\nprint(f\"Best Validation AUC at epoch {best_epoch + 1}: {best_val_auc:.5f}\")","metadata":{"_uuid":"05724957-43fc-4632-87a9-339e624a00a9","_cell_guid":"021a284b-f421-4adb-87eb-a6f91d148f90","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **FourierFusionNet **","metadata":{"_uuid":"55ba6d0d-a841-4d05-99d3-45b4fed4d1d6","_cell_guid":"58c44ac1-6e84-4e6d-b6b3-ae4b8000c40d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\n\ndef visualize_fft(image):\n    \"\"\"Compute and visualize FFT magnitude spectrum of a skin lesion image.\"\"\"\n    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n    f_transform = np.fft.fft2(gray_image)  # Apply 2D FFT\n    f_shift = np.fft.fftshift(f_transform)  # Shift zero frequency component to center\n    magnitude_spectrum = np.log(np.abs(f_shift) + 1)  # Compute magnitude and apply log scaling\n    \n    # Plot original image and its FFT spectrum\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1), plt.imshow(gray_image, cmap=\"gray\")\n    plt.title(\"Original Image\"), plt.axis(\"off\")\n\n    plt.subplot(1, 2, 2), plt.imshow(magnitude_spectrum, cmap=\"inferno\")\n    plt.title(\"FFT Magnitude Spectrum\"), plt.axis(\"off\")\n\n    plt.show()\n\n# Load example images of benign and malignant lesions\nbenign_image = cv2.imread(\"benign_example.jpg\")  # Replace with actual dataset image\nmalignant_image = cv2.imread(\"malignant_example.jpg\")\n\n# Visualize FFT for both cases\nvisualize_fft(benign_image)\nvisualize_fft(malignant_image)","metadata":{"_uuid":"069b8971-7552-4796-90a0-5114ecc2050b","_cell_guid":"7368ac8f-de3c-415c-8be5-5f2fc69a48b1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.metrics import Precision, Recall, AUC\nimport tensorflow.keras.backend as K\n\ndef fourier_transform_layer(image):\n    image_gray = tf.image.rgb_to_grayscale(image)\n    image_gray = tf.cast(image_gray, tf.float32)\n    fft = tf.signal.fft2d(tf.complex(image_gray[..., 0], tf.zeros_like(image_gray[..., 0])))\n    fft_magnitude = tf.abs(fft)\n    fft_magnitude = tf.math.log1p(fft_magnitude)\n    fft_magnitude = tf.expand_dims(fft_magnitude, axis=-1)\n    return tf.image.resize(fft_magnitude, (32, 32))\n\nclass AdaptiveFrequencyFiltering(keras.layers.Layer):\n    def __init__(self):\n        super(AdaptiveFrequencyFiltering, self).__init__()\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        self.filter_weights = self.add_weight(shape=(input_dim,), initializer=\"random_normal\", trainable=True)\n        self.bias = self.add_weight(shape=(input_dim,), initializer=\"zeros\", trainable=True)\n\n    def call(self, inputs):\n        return inputs * tf.sigmoid(self.filter_weights) + self.bias\n\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(71,), name=\"features\")\n\nbackbone = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(128, 128, 3))\n\nfor layer in backbone.layers[:100]:\n    layer.trainable = False\n\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)\n\nfourier_features = keras.layers.Lambda(fourier_transform_layer)(image_input)\nfourier_features = keras.layers.Conv2D(16, kernel_size=3, activation=\"relu\", padding=\"same\")(fourier_features)\nfourier_features = keras.layers.Conv2D(32, kernel_size=3, activation=\"relu\", padding=\"same\")(fourier_features)\nfourier_features = keras.layers.GlobalAveragePooling2D()(fourier_features)\nfourier_features = AdaptiveFrequencyFiltering()(fourier_features)\nfourier_features = keras.layers.Dense(128, activation=\"relu\")(fourier_features)\n\nx2 = keras.layers.Dense(128, activation=\"relu\")(feat_input)\nx2 = keras.layers.Dense(256, activation=\"relu\")(x2)\nx2 = keras.layers.BatchNormalization()(x2)\n\nconcat = keras.layers.Concatenate()([x1, fourier_features, x2])\n\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(concat)\n\nmodel = keras.models.Model(inputs={\"images\": image_input, \"features\": feat_input}, outputs=out)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n    metrics=[\n        AUC(name=\"auc\"),\n        Precision(name=\"precision\"),\n        Recall(name=\"recall\"),\n    ],\n)\n\nmodel.summary()\n\nhistory = model.fit(\n    training_ds,\n    epochs=10,\n    callbacks=[lr_cb, ckpt_cb],\n    validation_data=validation_ds,\n    verbose=1,\n    class_weight=class_weights,\n)","metadata":{"_uuid":"044174e7-7977-40ac-a02d-0d71ebc75840","_cell_guid":"b58fb282-6d57-4526-9114-604a1ae64780","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-25T08:16:08.698770Z","iopub.execute_input":"2025-04-25T08:16:08.699542Z","iopub.status.idle":"2025-04-25T08:20:07.446543Z","shell.execute_reply.started":"2025-04-25T08:16:08.699508Z","shell.execute_reply":"2025-04-25T08:20:07.445849Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"benchmark_model(model, model_name=\"FFT\", img_shape=(128,128,3), tabular_shape=(feature_space.get_encoded_features().shape[1],))","metadata":{"_uuid":"eefcb62c-efc8-4b14-b7ed-9af85db203d9","_cell_guid":"88d8d1f4-c851-4ceb-b3e7-4841825bfbf6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-25T08:21:36.050852Z","iopub.execute_input":"2025-04-25T08:21:36.051493Z","iopub.status.idle":"2025-04-25T08:21:45.190915Z","shell.execute_reply.started":"2025-04-25T08:21:36.051456Z","shell.execute_reply":"2025-04-25T08:21:45.190055Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_epoch = np.argmax(history['val_auc'])\n\nbest_precision = history['val_precision'][best_epoch]\nbest_recall = history['val_recall'][best_epoch]\nbest_val_auc = history['val_auc'][best_epoch]\n\nprint(f\"Best Precision: {best_precision:.5f}\")\nprint(f\"Best Recall: {best_recall:.5f}\")\nprint(f\"Best Validation AUC: {best_val_auc:.5f}\")","metadata":{"_uuid":"883ce613-e3ec-4ad9-8c1d-df87a1df21d1","_cell_guid":"3744d59c-e748-494d-9841-1f589c9208a9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-18T20:27:01.756641Z","iopub.execute_input":"2025-03-18T20:27:01.757317Z","iopub.status.idle":"2025-03-18T20:27:01.764046Z","shell.execute_reply.started":"2025-03-18T20:27:01.757285Z","shell.execute_reply":"2025-03-18T20:27:01.763124Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Symbolic Reasoning Model","metadata":{"_uuid":"484ecf94-8761-45b0-831c-2260de1dacef","_cell_guid":"22163c2d-aca7-40a7-97c0-cde75a989b72","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.metrics import Precision, Recall, AUC\nimport tensorflow.keras.backend as K\n\nclass SymbolicReasoningLayer(keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(SymbolicReasoningLayer, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        image_features, metadata_features = inputs\n        asymmetry_score = metadata_features[:, 0]\n        border_irregularity = metadata_features[:, 1]\n        rule_based_score = (asymmetry_score + border_irregularity) / 2\n        rule_based_score = tf.expand_dims(rule_based_score, axis=-1)\n        return tf.concat([image_features, rule_based_score], axis=-1)\n\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(71,), name=\"features\")\n\nbackbone = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\nx1 = backbone(image_input)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)\nx1 = keras.layers.Dense(256, activation=\"relu\")(x1)\n\nx2 = keras.layers.Dense(128, activation=\"relu\")(feat_input)\nx2 = keras.layers.Dense(256, activation=\"relu\")(x2)\nx2 = keras.layers.BatchNormalization()(x2)\n\nreasoning_out = SymbolicReasoningLayer()([x1, x2])\n\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(reasoning_out)\n\nmodel = keras.models.Model(inputs={\"images\": image_input, \"features\": feat_input}, outputs=out)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n    metrics=[AUC(name=\"auc\"), Precision(name=\"precision\"), Recall(name=\"recall\")],\n)\n\nmodel.summary()\n# Train the Model\nhistory = model.fit(\n    training_ds,\n    epochs=30,\n    callbacks=[lr_cb, ckpt_cb],\n    validation_data=validation_ds,\n    verbose=2,  \n    class_weight=class_weights\n)","metadata":{"_uuid":"fce57ad4-c9d3-4f4e-9fd5-e656cb187af0","_cell_guid":"2049ccba-0741-486b-adc2-987fc33b8e7a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-19T04:12:46.399149Z","iopub.execute_input":"2025-03-19T04:12:46.399505Z","iopub.status.idle":"2025-03-19T04:25:27.913370Z","shell.execute_reply.started":"2025-03-19T04:12:46.399464Z","shell.execute_reply":"2025-03-19T04:25:27.912514Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epoch = np.argmax(history['val_auc'])\n\nprecision = history['val_precision'][epoch]\nrecall = history['val_recall'][epoch]\nval_auc = history['val_auc'][epoch]\n\nprint(f\"Precision: {precision:.5f}\")\nprint(f\"Recall: {recall:.5f}\")\nprint(f\"Validation AUC: {val_auc:.5f}\")","metadata":{"_uuid":"734f4e66-dcc3-476c-8b4d-c2b499e8c1cd","_cell_guid":"8f972999-fc85-4688-93d3-d69594609d8a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-18T20:29:00.644642Z","iopub.execute_input":"2025-03-18T20:29:00.644987Z","iopub.status.idle":"2025-03-18T20:29:00.651291Z","shell.execute_reply.started":"2025-03-18T20:29:00.644957Z","shell.execute_reply":"2025-03-18T20:29:00.650376Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# WaveLet Model","metadata":{"_uuid":"7c137eb6-d500-4df5-8bf6-ddcbeba9d015","_cell_guid":"7dea5df5-ec6b-4067-a7d1-ee3c6b684978","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\n\nclass LearnableWaveletTransform(keras.layers.Layer):\n    def __init__(self, filters=32):\n        super(LearnableWaveletTransform, self).__init__()\n        self.filters = filters\n    def build(self, input_shape):\n        # Initialize low-pass and high-pass kernels with correct shape\n        low_pass_init = np.array([[1, 2, 1],\n                                  [2, 4, 2],\n                                  [1, 2, 1]]) / 16.0\n    \n        high_pass_init = np.array([[-1, 0, 1],\n                                    [-2, 0, 2],\n                                    [-1, 0, 1]])\n    \n        # Expand dims to match the required shape (3, 3, input_channels, output_filters)\n        low_pass_init = np.expand_dims(low_pass_init, axis=-1)  # (3, 3, 1)\n        low_pass_init = np.tile(low_pass_init, (1, 1, input_shape[-1]))  # (3, 3, input_channels)\n        low_pass_init = np.expand_dims(low_pass_init, axis=-1)  # (3, 3, input_channels, 1)\n        low_pass_init = np.tile(low_pass_init, (1, 1, 1, self.filters))  # (3, 3, input_channels, filters)\n    \n        high_pass_init = np.expand_dims(high_pass_init, axis=-1)\n        high_pass_init = np.tile(high_pass_init, (1, 1, input_shape[-1]))\n        high_pass_init = np.expand_dims(high_pass_init, axis=-1)\n        high_pass_init = np.tile(high_pass_init, (1, 1, 1, self.filters))\n    \n        # Ensure the initializer is properly shaped\n        self.low_pass_kernels = self.add_weight(\n            shape=(3, 3, input_shape[-1], self.filters),\n            initializer=tf.keras.initializers.Constant(low_pass_init),\n            trainable=True,\n            name=\"low_pass_kernels\"\n        )\n    \n        self.high_pass_kernels = self.add_weight(\n            shape=(3, 3, input_shape[-1], self.filters),\n            initializer=tf.keras.initializers.Constant(high_pass_init),\n            trainable=True,\n            name=\"high_pass_kernels\"\n        )\n\n\n    def call(self, inputs):\n        low_freq = tf.nn.conv2d(inputs, self.low_pass_kernels, strides=1, padding=\"SAME\")\n        high_freq = tf.nn.conv2d(inputs, self.high_pass_kernels, strides=1, padding=\"SAME\")\n\n        low_freq_downsampled = tf.nn.avg_pool(low_freq, ksize=2, strides=2, padding=\"SAME\")\n        high_freq_downsampled = tf.nn.avg_pool(high_freq, ksize=2, strides=2, padding=\"SAME\")\n\n        return tf.concat([low_freq_downsampled, high_freq_downsampled], axis=-1)\n\ndef squeeze_excitation_block(inputs, ratio=16):\n    filters = inputs.shape[-1]\n    se = keras.layers.GlobalAveragePooling2D()(inputs)\n    se = keras.layers.Dense(filters // ratio, activation=\"relu\")(se)\n    se = keras.layers.Dense(filters, activation=\"sigmoid\")(se)\n    return keras.layers.Multiply()([inputs, se])\n\nimage_input = keras.Input(shape=(128, 128, 3), name=\"images\")\nfeat_input = keras.Input(shape=(71,), name=\"features\")\n\nbackbone = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(128, 128, 3))\nfor layer in backbone.layers[:100]:\n    layer.trainable = False\n\nx1 = backbone(image_input)\nx1 = squeeze_excitation_block(x1)\nx1 = keras.layers.GlobalAveragePooling2D()(x1)\nx1 = keras.layers.BatchNormalization()(x1)\n\nwavelet_features = LearnableWaveletTransform(filters=32)(image_input)\nwavelet_features = keras.layers.Conv2D(16, kernel_size=3, activation=\"relu\", padding=\"same\")(wavelet_features)\nwavelet_features = keras.layers.Conv2D(32, kernel_size=3, activation=\"relu\", padding=\"same\")(wavelet_features)\nwavelet_features = keras.layers.GlobalAveragePooling2D()(wavelet_features)\nwavelet_features = keras.layers.Dense(128, activation=\"relu\")(wavelet_features)\n\nx2 = keras.layers.Dense(128, activation=\"relu\")(feat_input)\nx2 = keras.layers.Dense(256, activation=\"relu\")(x2)\nx2 = keras.layers.BatchNormalization()(x2)\n\nconcat = keras.layers.Concatenate()([x1, wavelet_features, x2])\n\nout = keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(concat)\n\nmodel = keras.models.Model(inputs={\"images\": image_input, \"features\": feat_input}, outputs=out)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n    metrics=[\n        AUC(name=\"auc\"),\n        Precision(name=\"precision\"),\n        Recall(name=\"recall\"),\n    ],\n)\n\nmodel.summary()\n\nhistory = model.fit(\n    training_ds,\n    epochs=30,\n    callbacks=[lr_cb, ckpt_cb],\n    validation_data=validation_ds,\n    verbose=2,\n    class_weight=class_weights,\n)","metadata":{"_uuid":"4b165507-af36-4cb9-9c65-aeecda3fc010","_cell_guid":"c050db3b-efd4-4e67-a436-5a2e7a9969e0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-19T04:48:44.714322Z","iopub.execute_input":"2025-03-19T04:48:44.714722Z","iopub.status.idle":"2025-03-19T04:58:34.910000Z","shell.execute_reply.started":"2025-03-19T04:48:44.714691Z","shell.execute_reply":"2025-03-19T04:58:34.908953Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nepoch = np.argmax(history['val_auc'])\n\nprecision = history['val_precision'][epoch]\nrecall = history['val_recall'][epoch]\nval_auc = history['val_auc'][epoch]\n\nprint(f\"Precision: {precision:.5f}\")\nprint(f\"Recall: {recall:.5f}\")\nprint(f\"Validation AUC: {val_auc:.5f}\")","metadata":{"_uuid":"2478405b-69f2-4017-a8c6-327056b982fd","_cell_guid":"b665c13f-2d9c-4f9a-ae19-64c126c83f6b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-18T20:31:06.182704Z","iopub.execute_input":"2025-03-18T20:31:06.183280Z","iopub.status.idle":"2025-03-18T20:31:06.189229Z","shell.execute_reply.started":"2025-03-18T20:31:06.183248Z","shell.execute_reply":"2025-03-18T20:31:06.188376Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}